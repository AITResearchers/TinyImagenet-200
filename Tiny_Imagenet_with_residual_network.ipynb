{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Tiny Imagenet with residual network.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "DB4O1JdMJSHT",
        "colab_type": "code",
        "outputId": "2e10ce86-621e-4793-c598-2a10019edca6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 217
        }
      },
      "source": [
        " !wget http://cs231n.stanford.edu/tiny-imagenet-200.zip"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2019-04-10 11:14:45--  http://cs231n.stanford.edu/tiny-imagenet-200.zip\n",
            "Resolving cs231n.stanford.edu (cs231n.stanford.edu)... 171.64.68.10\n",
            "Connecting to cs231n.stanford.edu (cs231n.stanford.edu)|171.64.68.10|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 248100043 (237M) [application/zip]\n",
            "Saving to: ‘tiny-imagenet-200.zip’\n",
            "\n",
            "tiny-imagenet-200.z 100%[===================>] 236.61M  61.0MB/s    in 4.7s    \n",
            "\n",
            "2019-04-10 11:14:50 (50.0 MB/s) - ‘tiny-imagenet-200.zip’ saved [248100043/248100043]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pRZnH_QdJVPc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!unzip -qq 'tiny-imagenet-200.zip'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z1s0dtZV0CFh",
        "colab_type": "code",
        "outputId": "2e877d94-1656-4d31-d51e-cf58dcd794d5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 128
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dvt-LLi2zlpF",
        "colab_type": "code",
        "outputId": "501d1a05-caba-4396-bfe3-d9f351b27acd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from keras.models import Model\n",
        "from keras.layers import (\n",
        "    Input,\n",
        "    Activation,\n",
        "    Dense,\n",
        "    Flatten, Dropout, Concatenate\n",
        ")\n",
        "from keras.layers.convolutional import (\n",
        "    Conv2D,\n",
        "    MaxPooling2D,\n",
        "    AveragePooling2D\n",
        ")\n",
        "from keras.layers.merge import add\n",
        "from keras.layers.normalization import BatchNormalization\n",
        "from keras.regularizers import l2,l1\n",
        "from keras import backend as K\n",
        "from keras.layers import Activation"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ApjKq-NEH_7g",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "from keras import layers\n",
        "from keras.layers import Input, Add, Dense, Activation, ZeroPadding2D, BatchNormalization, Flatten, Conv2D, AveragePooling2D, MaxPooling2D, GlobalMaxPooling2D,GlobalAveragePooling2D\n",
        "from keras.models import Model, load_model\n",
        "from keras.preprocessing import image\n",
        "from keras.utils import layer_utils\n",
        "from keras.utils.data_utils import get_file\n",
        "from keras.applications.imagenet_utils import preprocess_input\n",
        "import pydot\n",
        "from IPython.display import SVG\n",
        "from keras.utils.vis_utils import model_to_dot\n",
        "from keras.utils import plot_model\n",
        "#from resnets_utils import *\n",
        "from keras.initializers import glorot_uniform\n",
        "import scipy.misc\n",
        "from matplotlib.pyplot import imshow\n",
        "%matplotlib inline\n",
        "from keras import metrics\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "import keras.backend as K\n",
        "K.set_image_data_format('channels_last')\n",
        "K.set_learning_phase(1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QpWrqthyIEob",
        "colab_type": "code",
        "outputId": "f0850ab8-85aa-4363-e87e-23d081e0cec3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "val_data = pd.read_csv('./tiny-imagenet-200/val/val_annotations.txt', sep='\\t', header=None, names=['File', 'Class', 'X', 'Y', 'H', 'W'])\n",
        "val_data.drop(['X', 'Y', 'H', 'W'], axis=1, inplace=True)\n",
        "val_data.head(3)\n",
        "train_datagen = ImageDataGenerator(\n",
        "        featurewise_center=True,  # set input mean to 0 over the dataset\n",
        "        samplewise_center=True,  # set each sample mean to 0\n",
        "        rescale = 1./255,\n",
        "        featurewise_std_normalization=False,  # divide inputs by std of the dataset\n",
        "        samplewise_std_normalization=False,  # divide each input by its std\n",
        "        zca_whitening=False,  # apply ZCA whitening\n",
        "        rotation_range=30,  # randomly rotate images in the range (degrees, 0 to 180)\n",
        "        width_shift_range=0.1,  # randomly shift images horizontally (fraction of total width)\n",
        "        height_shift_range=0.1,  # randomly shift images vertically (fraction of total height)\n",
        "        horizontal_flip=True,  # randomly flip images\n",
        "        vertical_flip=True # randomly flip images\n",
        "    )\n",
        "\n",
        "valid_datagen = ImageDataGenerator(rescale=1./255)\n",
        "train_generator = train_datagen.flow_from_directory( r'./tiny-imagenet-200/train/', target_size=(64,64), color_mode='rgb', \n",
        "                                                    batch_size=250, class_mode='categorical', seed=1)\n",
        "validation_generator = valid_datagen.flow_from_dataframe(val_data, directory='./tiny-imagenet-200/val/images/', x_col='File', y_col='Class', target_size=(64,64),\n",
        "                                                    color_mode='rgb', class_mode='categorical', batch_size=64, shuffle=True, seed=42)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 100000 images belonging to 200 classes.\n",
            "Found 10000 images belonging to 200 classes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5bFem55pCOfY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_model(inp_shape=(32,32,3)):\n",
        "    from keras.models import Sequential\n",
        "    from keras.layers.advanced_activations import LeakyReLU , ReLU\n",
        "    from keras.layers import Reshape, Activation, Conv2D, Input, MaxPooling2D, BatchNormalization, Flatten, Dense, Lambda, GlobalMaxPooling2D\n",
        "    from keras import initializers\n",
        "    from keras.regularizers import l2\n",
        "    from keras.layers.merge import concatenate\n",
        "    from keras.models import Sequential\n",
        "    from keras.layers.advanced_activations import LeakyReLU , ReLU\n",
        "    from keras.layers import Reshape, Activation, Conv2D, Input, MaxPooling2D, BatchNormalization, Flatten, Dense, Lambda, GlobalMaxPooling2D\n",
        "    from keras import initializers\n",
        "    from keras.regularizers import l2\n",
        "    from keras.layers.merge import concatenate\n",
        "    from keras.optimizers import SGD, Adam\n",
        "    input = Input(shape=inp_shape)\n",
        "\n",
        "    layer1 = Conv2D(32, (3,3), strides=(1,1), padding='same', name='conv_1', use_bias=False,kernel_initializer=\"glorot_normal\",\n",
        "                    kernel_regularizer=l2(5e-4))(input)\n",
        "    layer1 = BatchNormalization(name='norm_1')(layer1)\n",
        "    layer1 = ReLU()(layer1)\n",
        "    layer2 = Conv2D(64, (3,3), strides=(1,1), padding='same', name='conv_2', use_bias=False,kernel_initializer=\"glorot_normal\",\n",
        "                    kernel_regularizer=l2(5e-4))(layer1)\n",
        "    layer2 = BatchNormalization(name='norm_2')(layer2)\n",
        "    layer2 = ReLU()(layer2)\n",
        "    layer3 = Conv2D(128, (3,3), strides=(1,1), padding='same', name='conv_3', use_bias=False,kernel_initializer=\"glorot_normal\",\n",
        "                    kernel_regularizer=l2(5e-4))(layer2)\n",
        "    layer3 = BatchNormalization(name='norm_3')(layer3)\n",
        "    layer3 = ReLU()(layer3)\n",
        "    #con_1 = concatenate([layer3,layer1],name='con_1')\n",
        "    layer4 = Conv2D(256, (3,3), strides=(1,1), padding='same', name='conv_4', use_bias=False,kernel_initializer=\"glorot_normal\",\n",
        "                    kernel_regularizer=l2(5e-4))(layer3)\n",
        "    layer4 = BatchNormalization(name='norm_4')(layer4)\n",
        "    layer4 = ReLU()(layer4)\n",
        "    con_2 = concatenate([layer4,layer1],name='con_2')\n",
        "    MP_1 = MaxPooling2D(pool_size=(2, 2))(con_2)\n",
        "    layer6 = Conv2D(128, (1,1), strides=(1,1), padding='same', name='BN1_conv_1x1', use_bias=False,kernel_initializer=\"glorot_normal\",\n",
        "                    kernel_regularizer=l2(5e-4))(MP_1)\n",
        "    layer6 = BatchNormalization(name='BN1_norm_1')(layer6)\n",
        "    layer6 = ReLU()(layer6)\n",
        "    #MaxPooling2D(pool_size=(2, 2))(layer11)\n",
        "    #Block 3\n",
        "    layer7 = Conv2D(128, (3,3), strides=(1,1), padding='same', name='conv_6', use_bias=False,kernel_initializer=\"glorot_normal\",\n",
        "                    kernel_regularizer=l2(5e-4))(layer6)\n",
        "    layer7 = BatchNormalization(name='norm_6')(layer7)\n",
        "    layer7 = ReLU()(layer7)\n",
        "    layer8 = Conv2D(256, (3,3), strides=(1,1), padding='same', name='conv_7', use_bias=False,kernel_initializer=\"glorot_normal\",\n",
        "                    kernel_regularizer=l2(5e-4))(layer7)\n",
        "    layer8 = BatchNormalization(name='norm_7')(layer8)\n",
        "    layer8 = ReLU()(layer8)\n",
        "    #con_3 =  concatenate([layer6,layer8],name='con_3')\n",
        "    MP_2 = MaxPooling2D(pool_size=(2, 2))(layer8)\n",
        "    layer9 = Conv2D(128, (1,1), strides=(1,1), padding='same', name='BN2_conv_1x1', use_bias=False,kernel_initializer=\"glorot_normal\",\n",
        "                    kernel_regularizer=l2(5e-4))(MP_2)\n",
        "    layer9 = BatchNormalization(name='BN2_norm_2')(layer9)\n",
        "    layer9 = ReLU()(layer9)\n",
        "    #layer9 = MaxPooling2D(pool_size=(2, 2))(layer9)\n",
        "    #Block 4\n",
        "    layer10 = Conv2D(256, (3,3), strides=(1,1), padding='same', name='conv_8', use_bias=False,kernel_initializer=\"glorot_normal\",\n",
        "                    kernel_regularizer=l2(5e-4))(layer9)\n",
        "    layer10 = BatchNormalization(name='norm_8')(layer10)\n",
        "    layer10 = ReLU()(layer10)\n",
        "    layer11 = Conv2D(512, (3,1), strides=(1,1), padding='same', name='conv_9', use_bias=False,kernel_initializer=\"glorot_normal\",\n",
        "                    kernel_regularizer=l2(5e-4))(layer10)\n",
        "    layer11 = Conv2D(512, (1,3), strides=(1,1), padding='same', name='conv_9a', use_bias=False,kernel_initializer=\"glorot_normal\",\n",
        "                    kernel_regularizer=l2(5e-4))(layer11)\n",
        "    layer11 = BatchNormalization(name='norm_9')(layer11)\n",
        "    layer11 = ReLU()(layer11)\n",
        "    #con_4 = concatenate([layer9,layer6],name='con_4')\n",
        "    MP_3 = MaxPooling2D(pool_size=(2, 2))(layer11)\n",
        "    layer12 = Conv2D(256, (1,1), strides=(1,1), padding='same', name='BN3_conv_1x1', use_bias=False,kernel_initializer=\"glorot_normal\",\n",
        "                    kernel_regularizer=l2(5e-4))(MP_3)\n",
        "    layer12 = BatchNormalization(name='BN3_norm_3')(layer12)\n",
        "    layer12 = ReLU()(layer12)\n",
        "    layer13 = Conv2D(256, (3,3), strides=(1,1), padding='same', name='conv_10', use_bias=False,kernel_initializer=\"glorot_normal\",\n",
        "                    kernel_regularizer=l2(5e-4))(layer12)\n",
        "    layer13 = BatchNormalization(name='norm_10')(layer13)\n",
        "    layer13 = ReLU()(layer13)\n",
        "    layer14 = Conv2D(512, (3,1), strides=(1,1), padding='same', name='conv_11', use_bias=False,kernel_initializer=\"glorot_normal\",\n",
        "                    kernel_regularizer=l2(5e-4))(layer13)\n",
        "    layer14 = Conv2D(512, (1,3), strides=(1,1), padding='same', name='conv_11a', use_bias=False,kernel_initializer=\"glorot_normal\",\n",
        "                    kernel_regularizer=l2(5e-4))(layer14)\n",
        "    layer14 = BatchNormalization(name='norm_11')(layer14)\n",
        "    layer14 = ReLU()(layer14)\n",
        "    con_5 = concatenate([layer12,layer14],name='con_5')\n",
        "    MP_4 = MaxPooling2D(pool_size=(2, 2))(con_5)\n",
        "   \n",
        "    layer15 = Conv2D(512, (1,1), strides=(1,1), padding='same', name='BN4_conv_1x1', use_bias=False,kernel_initializer=\"glorot_normal\",\n",
        "                    kernel_regularizer=l2(5e-4))(MP_4)\n",
        "    layer15 = BatchNormalization(name='BN4_norm_4')(layer15)\n",
        "    layer15 = ReLU()(layer15)\n",
        "    layer16 = Conv2D(256, (3,1), strides=(1,1), padding='same', name='conv_12', use_bias=False,kernel_initializer=\"glorot_normal\",\n",
        "                    kernel_regularizer=l2(5e-4))(layer15)\n",
        "    layer16 = Conv2D(256, (1,3), strides=(1,1), padding='same', name='conv_12a', use_bias=False,kernel_initializer=\"glorot_normal\",\n",
        "                    kernel_regularizer=l2(5e-4))(layer16)\n",
        "    layer16 = BatchNormalization(name='norm_12')(layer16)\n",
        "    layer16 = ReLU()(layer16)\n",
        "    layer17 = Conv2D(256, (3,3), strides=(1,1), padding='same', name='conv_13', use_bias=False,kernel_initializer=\"glorot_normal\",\n",
        "                    kernel_regularizer=l2(5e-4))(layer16)\n",
        "    layer17 = BatchNormalization(name='norm_13')(layer17)\n",
        "    layer17 = ReLU()(layer17)\n",
        "    #con_6 = concatenate([layer15,layer17],name='con_6')\n",
        "    MP_5 = MaxPooling2D()(layer17)\n",
        "    layer19 = Conv2D(200, (2,2), strides=(1,1), padding='valid', name='conv_flat', use_bias=False,kernel_initializer=\"glorot_normal\",\n",
        "                     kernel_regularizer=l2(5e-4))(MP_5)\n",
        "    layer19 = BatchNormalization(name='norm_flat')(layer19)\n",
        "    layer19 = Flatten()(layer19)\n",
        "    output = Activation('softmax')(layer19)\n",
        "    from keras.models import Model\n",
        "    model = Model(inputs=[input], outputs=[output])\n",
        "    #output = Activation('softmax')(layer19)\n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UMKyQfDO-ZVf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.callbacks import *\n",
        "\n",
        "class CyclicLR(Callback):\n",
        "    \"\"\"This callback implements a cyclical learning rate policy (CLR).\n",
        "    The method cycles the learning rate between two boundaries with\n",
        "    some constant frequency, as detailed in this paper (https://arxiv.org/abs/1506.01186).\n",
        "    The amplitude of the cycle can be scaled on a per-iteration or \n",
        "    per-cycle basis.\n",
        "    This class has three built-in policies, as put forth in the paper.\n",
        "    \"triangular\":\n",
        "        A basic triangular cycle w/ no amplitude scaling.\n",
        "    \"triangular2\":\n",
        "        A basic triangular cycle that scales initial amplitude by half each cycle.\n",
        "    \"exp_range\":\n",
        "        A cycle that scales initial amplitude by gamma**(cycle iterations) at each \n",
        "        cycle iteration.\n",
        "    For more detail, please see paper.\n",
        "    \n",
        "    # Example\n",
        "        ```python\n",
        "            clr = CyclicLR(base_lr=0.001, max_lr=0.006,\n",
        "                                step_size=2000., mode='triangular')\n",
        "            model.fit(X_train, Y_train, callbacks=[clr])\n",
        "        ```\n",
        "    \n",
        "    Class also supports custom scaling functions:\n",
        "        ```python\n",
        "            clr_fn = lambda x: 0.5*(1+np.sin(x*np.pi/2.))\n",
        "            clr = CyclicLR(base_lr=0.001, max_lr=0.006,\n",
        "                                step_size=2000., scale_fn=clr_fn,\n",
        "                                scale_mode='cycle')\n",
        "            model.fit(X_train, Y_train, callbacks=[clr])\n",
        "        ```    \n",
        "    # Arguments\n",
        "        base_lr: initial learning rate which is the\n",
        "            lower boundary in the cycle.\n",
        "        max_lr: upper boundary in the cycle. Functionally,\n",
        "            it defines the cycle amplitude (max_lr - base_lr).\n",
        "            The lr at any cycle is the sum of base_lr\n",
        "            and some scaling of the amplitude; therefore \n",
        "            max_lr may not actually be reached depending on\n",
        "            scaling function.\n",
        "        step_size: number of training iterations per\n",
        "            half cycle. Authors suggest setting step_size\n",
        "            2-8 x training iterations in epoch.\n",
        "        mode: one of {triangular, triangular2, exp_range}.\n",
        "            Default 'triangular'.\n",
        "            Values correspond to policies detailed above.\n",
        "            If scale_fn is not None, this argument is ignored.\n",
        "        gamma: constant in 'exp_range' scaling function:\n",
        "            gamma**(cycle iterations)\n",
        "        scale_fn: Custom scaling policy defined by a single\n",
        "            argument lambda function, where \n",
        "            0 <= scale_fn(x) <= 1 for all x >= 0.\n",
        "            mode paramater is ignored \n",
        "        scale_mode: {'cycle', 'iterations'}.\n",
        "            Defines whether scale_fn is evaluated on \n",
        "            cycle number or cycle iterations (training\n",
        "            iterations since start of cycle). Default is 'cycle'.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, base_lr=0.001, max_lr=0.006, step_size=2000., mode='triangular',\n",
        "                 gamma=1., scale_fn=None, scale_mode='cycle'):\n",
        "        super(CyclicLR, self).__init__()\n",
        "\n",
        "        self.base_lr = base_lr\n",
        "        self.max_lr = max_lr\n",
        "        self.step_size = step_size\n",
        "        self.mode = mode\n",
        "        self.gamma = gamma\n",
        "        if scale_fn == None:\n",
        "            if self.mode == 'triangular':\n",
        "                self.scale_fn = lambda x: 1.\n",
        "                self.scale_mode = 'cycle'\n",
        "            elif self.mode == 'triangular2':\n",
        "                self.scale_fn = lambda x: 1/(2.**(x-1))\n",
        "                self.scale_mode = 'cycle'\n",
        "            elif self.mode == 'exp_range':\n",
        "                self.scale_fn = lambda x: gamma**(x)\n",
        "                self.scale_mode = 'iterations'\n",
        "        else:\n",
        "            self.scale_fn = scale_fn\n",
        "            self.scale_mode = scale_mode\n",
        "        self.clr_iterations = 0.\n",
        "        self.trn_iterations = 0.\n",
        "        self.history = {}\n",
        "\n",
        "        self._reset()\n",
        "\n",
        "    def _reset(self, new_base_lr=None, new_max_lr=None,\n",
        "               new_step_size=None):\n",
        "        \"\"\"Resets cycle iterations.\n",
        "        Optional boundary/step size adjustment.\n",
        "        \"\"\"\n",
        "        if new_base_lr != None:\n",
        "            self.base_lr = new_base_lr\n",
        "        if new_max_lr != None:\n",
        "            self.max_lr = new_max_lr\n",
        "        if new_step_size != None:\n",
        "            self.step_size = new_step_size\n",
        "        self.clr_iterations = 0.\n",
        "        \n",
        "    def clr(self):\n",
        "        cycle = np.floor(1+self.clr_iterations/(2*self.step_size))\n",
        "        x = np.abs(self.clr_iterations/self.step_size - 2*cycle + 1)\n",
        "        if self.scale_mode == 'cycle':\n",
        "            return self.base_lr + (self.max_lr-self.base_lr)*np.maximum(0, (1-x))*self.scale_fn(cycle)\n",
        "        else:\n",
        "            return self.base_lr + (self.max_lr-self.base_lr)*np.maximum(0, (1-x))*self.scale_fn(self.clr_iterations)\n",
        "        \n",
        "    def on_train_begin(self, logs={}):\n",
        "        logs = logs or {}\n",
        "\n",
        "        if self.clr_iterations == 0:\n",
        "            K.set_value(self.model.optimizer.lr, self.base_lr)\n",
        "        else:\n",
        "            K.set_value(self.model.optimizer.lr, self.clr())        \n",
        "            \n",
        "    def on_batch_end(self, epoch, logs=None):\n",
        "        \n",
        "        logs = logs or {}\n",
        "        self.trn_iterations += 1\n",
        "        self.clr_iterations += 1\n",
        "\n",
        "        self.history.setdefault('lr', []).append(K.get_value(self.model.optimizer.lr))\n",
        "        self.history.setdefault('iterations', []).append(self.trn_iterations)\n",
        "\n",
        "        for k, v in logs.items():\n",
        "            self.history.setdefault(k, []).append(v)\n",
        "        \n",
        "        K.set_value(self.model.optimizer.lr, self.clr())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FqF2KZSr1g-3",
        "colab_type": "code",
        "outputId": "28ba4c4a-e798-49a1-b686-e959cd54f2ef",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 2383
        }
      },
      "source": [
        "model = create_model((64,64,3))\n",
        "plot_model(model, to_file='model_plot.png', show_shapes=True, show_layer_names=True)\n",
        "model.summary()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            (None, 64, 64, 3)    0                                            \n",
            "__________________________________________________________________________________________________\n",
            "conv_1 (Conv2D)                 (None, 64, 64, 32)   864         input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "norm_1 (BatchNormalization)     (None, 64, 64, 32)   128         conv_1[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "re_lu_1 (ReLU)                  (None, 64, 64, 32)   0           norm_1[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv_2 (Conv2D)                 (None, 64, 64, 64)   18432       re_lu_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "norm_2 (BatchNormalization)     (None, 64, 64, 64)   256         conv_2[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "re_lu_2 (ReLU)                  (None, 64, 64, 64)   0           norm_2[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv_3 (Conv2D)                 (None, 64, 64, 128)  73728       re_lu_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "norm_3 (BatchNormalization)     (None, 64, 64, 128)  512         conv_3[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "re_lu_3 (ReLU)                  (None, 64, 64, 128)  0           norm_3[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv_4 (Conv2D)                 (None, 64, 64, 256)  294912      re_lu_3[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "norm_4 (BatchNormalization)     (None, 64, 64, 256)  1024        conv_4[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "re_lu_4 (ReLU)                  (None, 64, 64, 256)  0           norm_4[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "con_2 (Concatenate)             (None, 64, 64, 288)  0           re_lu_4[0][0]                    \n",
            "                                                                 re_lu_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_1 (MaxPooling2D)  (None, 32, 32, 288)  0           con_2[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "BN1_conv_1x1 (Conv2D)           (None, 32, 32, 128)  36864       max_pooling2d_1[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "BN1_norm_1 (BatchNormalization) (None, 32, 32, 128)  512         BN1_conv_1x1[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "re_lu_5 (ReLU)                  (None, 32, 32, 128)  0           BN1_norm_1[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv_6 (Conv2D)                 (None, 32, 32, 128)  147456      re_lu_5[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "norm_6 (BatchNormalization)     (None, 32, 32, 128)  512         conv_6[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "re_lu_6 (ReLU)                  (None, 32, 32, 128)  0           norm_6[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv_7 (Conv2D)                 (None, 32, 32, 256)  294912      re_lu_6[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "norm_7 (BatchNormalization)     (None, 32, 32, 256)  1024        conv_7[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "re_lu_7 (ReLU)                  (None, 32, 32, 256)  0           norm_7[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_2 (MaxPooling2D)  (None, 16, 16, 256)  0           re_lu_7[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "BN2_conv_1x1 (Conv2D)           (None, 16, 16, 128)  32768       max_pooling2d_2[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "BN2_norm_2 (BatchNormalization) (None, 16, 16, 128)  512         BN2_conv_1x1[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "re_lu_8 (ReLU)                  (None, 16, 16, 128)  0           BN2_norm_2[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv_8 (Conv2D)                 (None, 16, 16, 256)  294912      re_lu_8[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "norm_8 (BatchNormalization)     (None, 16, 16, 256)  1024        conv_8[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "re_lu_9 (ReLU)                  (None, 16, 16, 256)  0           norm_8[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv_9 (Conv2D)                 (None, 16, 16, 512)  393216      re_lu_9[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv_9a (Conv2D)                (None, 16, 16, 512)  786432      conv_9[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "norm_9 (BatchNormalization)     (None, 16, 16, 512)  2048        conv_9a[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "re_lu_10 (ReLU)                 (None, 16, 16, 512)  0           norm_9[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_3 (MaxPooling2D)  (None, 8, 8, 512)    0           re_lu_10[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "BN3_conv_1x1 (Conv2D)           (None, 8, 8, 256)    131072      max_pooling2d_3[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "BN3_norm_3 (BatchNormalization) (None, 8, 8, 256)    1024        BN3_conv_1x1[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "re_lu_11 (ReLU)                 (None, 8, 8, 256)    0           BN3_norm_3[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv_10 (Conv2D)                (None, 8, 8, 256)    589824      re_lu_11[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "norm_10 (BatchNormalization)    (None, 8, 8, 256)    1024        conv_10[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "re_lu_12 (ReLU)                 (None, 8, 8, 256)    0           norm_10[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv_11 (Conv2D)                (None, 8, 8, 512)    393216      re_lu_12[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "conv_11a (Conv2D)               (None, 8, 8, 512)    786432      conv_11[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "norm_11 (BatchNormalization)    (None, 8, 8, 512)    2048        conv_11a[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "re_lu_13 (ReLU)                 (None, 8, 8, 512)    0           norm_11[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "con_5 (Concatenate)             (None, 8, 8, 768)    0           re_lu_11[0][0]                   \n",
            "                                                                 re_lu_13[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_4 (MaxPooling2D)  (None, 4, 4, 768)    0           con_5[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "BN4_conv_1x1 (Conv2D)           (None, 4, 4, 512)    393216      max_pooling2d_4[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "BN4_norm_4 (BatchNormalization) (None, 4, 4, 512)    2048        BN4_conv_1x1[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "re_lu_14 (ReLU)                 (None, 4, 4, 512)    0           BN4_norm_4[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv_12 (Conv2D)                (None, 4, 4, 256)    393216      re_lu_14[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "conv_12a (Conv2D)               (None, 4, 4, 256)    196608      conv_12[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "norm_12 (BatchNormalization)    (None, 4, 4, 256)    1024        conv_12a[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "re_lu_15 (ReLU)                 (None, 4, 4, 256)    0           norm_12[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv_13 (Conv2D)                (None, 4, 4, 256)    589824      re_lu_15[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "norm_13 (BatchNormalization)    (None, 4, 4, 256)    1024        conv_13[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "re_lu_16 (ReLU)                 (None, 4, 4, 256)    0           norm_13[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_5 (MaxPooling2D)  (None, 2, 2, 256)    0           re_lu_16[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "conv_flat (Conv2D)              (None, 1, 1, 200)    204800      max_pooling2d_5[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "norm_flat (BatchNormalization)  (None, 1, 1, 200)    800         conv_flat[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "flatten_1 (Flatten)             (None, 200)          0           norm_flat[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_1 (Activation)       (None, 200)          0           flatten_1[0][0]                  \n",
            "==================================================================================================\n",
            "Total params: 6,069,248\n",
            "Trainable params: 6,060,976\n",
            "Non-trainable params: 8,272\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4oh-_bdnTUNT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.optimizers import *\n",
        "clr_triangular = CyclicLR(base_lr=0.0001,max_lr=0.001,step_size=2000,mode='triangular')\n",
        "adm=SGD(lr=0.0001, decay=1e-5, momentum=0.95, nesterov=True)\n",
        "model.compile(loss='categorical_crossentropy', metrics=['accuracy'],optimizer=adm)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0ds38Ln5crlN",
        "colab_type": "code",
        "outputId": "fd06153a-73f1-4266-f327-7dc6e0e055fe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 7437
        }
      },
      "source": [
        "from keras.callbacks import ReduceLROnPlateau, CSVLogger, EarlyStopping,ModelCheckpoint\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.models import load_model\n",
        "data_augmentation = True\n",
        "batch_size = 128\n",
        "epoch = 2\n",
        "from keras.utils import np_utils\n",
        "#call_backs\n",
        "lr_reducer = ReduceLROnPlateau(factor=np.sqrt(0.1), cooldown=0, patience=5, min_lr=0.5e-6)\n",
        "early_stopper = EarlyStopping(min_delta=0.001, patience=5)\n",
        "filepath=\"drive/My Drive/weights_folder/weights_best_new2.h5\"\n",
        "checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
        "callbacks_list = [checkpoint, lr_reducer,clr_triangular]\n",
        "\n",
        "\n",
        "if not data_augmentation:\n",
        "    print('Not using data augmentation.')\n",
        "    model.fit(X_train, Y_train,\n",
        "              batch_size=batch_size,\n",
        "              epochs=epoch,\n",
        "              validation_data=(X_test, Y_test),\n",
        "              shuffle=True,\n",
        "              callbacks=callbacks_list)\n",
        "else:\n",
        "    print('Using real-time data augmentation.')\n",
        "    # This will do preprocessing and realtime data augmentation:\n",
        "    datagen = ImageDataGenerator(\n",
        "        featurewise_center=True,  # set input mean to 0 over the dataset\n",
        "        samplewise_center=True,  # set each sample mean to 0\n",
        "        featurewise_std_normalization=False,  # divide inputs by std of the dataset\n",
        "        samplewise_std_normalization=False,  # divide each input by its std\n",
        "        zca_whitening=False,  # apply ZCA whitening\n",
        "        rotation_range=45,  # randomly rotate images in the range (degrees, 0 to 180)\n",
        "        width_shift_range=0.1,  # randomly shift images horizontally (fraction of total width)\n",
        "        height_shift_range=0.1,  # randomly shift images vertically (fraction of total height)\n",
        "        horizontal_flip=True,  # randomly flip images\n",
        "        vertical_flip=False)  # randomly flip images\n",
        "\n",
        "    for i in range(epoch):\n",
        "      model.fit_generator(train_generator,\n",
        "                          steps_per_epoch=200,\n",
        "                          validation_data=validation_generator, validation_steps=100,\n",
        "                          epochs=50, verbose=1, max_q_size=100,callbacks=callbacks_list)\n",
        "    print(\"Saving model to disk\")\n",
        "    model.save(filepath)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using real-time data augmentation.\n",
            "Epoch 1/50\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:43: UserWarning: Update your `fit_generator` call to the Keras 2 API: `fit_generator(<keras_pre..., steps_per_epoch=200, validation_data=<keras_pre..., validation_steps=100, epochs=50, verbose=1, callbacks=[<keras.ca..., max_queue_size=100)`\n",
            "/usr/local/lib/python3.6/dist-packages/keras_preprocessing/image/image_data_generator.py:699: UserWarning: This ImageDataGenerator specifies `featurewise_center`, but it hasn't been fit on any training data. Fit it first by calling `.fit(numpy_data)`.\n",
            "  warnings.warn('This ImageDataGenerator specifies '\n",
            "/usr/local/lib/python3.6/dist-packages/keras_preprocessing/image/image_data_generator.py:707: UserWarning: This ImageDataGenerator specifies `featurewise_std_normalization`, but it hasn't been fit on any training data. Fit it first by calling `.fit(numpy_data)`.\n",
            "  warnings.warn('This ImageDataGenerator specifies '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "200/200 [==============================] - 410s 2s/step - loss: 8.0026 - acc: 0.0130 - val_loss: 7.9221 - val_acc: 0.0140\n",
            "\n",
            "Epoch 00001: val_acc improved from -inf to 0.01401, saving model to drive/My Drive/weights_folder/weights_best_new2.h5\n",
            "Epoch 2/50\n",
            "200/200 [==============================] - 403s 2s/step - loss: 7.7024 - acc: 0.0263 - val_loss: 7.6922 - val_acc: 0.0233\n",
            "\n",
            "Epoch 00002: val_acc improved from 0.01401 to 0.02328, saving model to drive/My Drive/weights_folder/weights_best_new2.h5\n",
            "Epoch 3/50\n",
            "200/200 [==============================] - 403s 2s/step - loss: 7.4859 - acc: 0.0426 - val_loss: 7.4656 - val_acc: 0.0381\n",
            "\n",
            "Epoch 00003: val_acc improved from 0.02328 to 0.03810, saving model to drive/My Drive/weights_folder/weights_best_new2.h5\n",
            "Epoch 4/50\n",
            "200/200 [==============================] - 403s 2s/step - loss: 7.2768 - acc: 0.0601 - val_loss: 7.3011 - val_acc: 0.0497\n",
            "\n",
            "Epoch 00004: val_acc improved from 0.03810 to 0.04975, saving model to drive/My Drive/weights_folder/weights_best_new2.h5\n",
            "Epoch 5/50\n",
            "200/200 [==============================] - 403s 2s/step - loss: 7.0847 - acc: 0.0762 - val_loss: 7.1391 - val_acc: 0.0669\n",
            "\n",
            "Epoch 00005: val_acc improved from 0.04975 to 0.06688, saving model to drive/My Drive/weights_folder/weights_best_new2.h5\n",
            "Epoch 6/50\n",
            "200/200 [==============================] - 403s 2s/step - loss: 6.9094 - acc: 0.1003 - val_loss: 6.9892 - val_acc: 0.0864\n",
            "\n",
            "Epoch 00006: val_acc improved from 0.06688 to 0.08643, saving model to drive/My Drive/weights_folder/weights_best_new2.h5\n",
            "Epoch 7/50\n",
            "200/200 [==============================] - 402s 2s/step - loss: 6.7721 - acc: 0.1179 - val_loss: 6.8548 - val_acc: 0.0967\n",
            "\n",
            "Epoch 00007: val_acc improved from 0.08643 to 0.09666, saving model to drive/My Drive/weights_folder/weights_best_new2.h5\n",
            "Epoch 8/50\n",
            "200/200 [==============================] - 402s 2s/step - loss: 6.6135 - acc: 0.1399 - val_loss: 6.7337 - val_acc: 0.1123\n",
            "\n",
            "Epoch 00008: val_acc improved from 0.09666 to 0.11234, saving model to drive/My Drive/weights_folder/weights_best_new2.h5\n",
            "Epoch 9/50\n",
            "200/200 [==============================] - 402s 2s/step - loss: 6.5033 - acc: 0.1542 - val_loss: 6.6082 - val_acc: 0.1277\n",
            "\n",
            "Epoch 00009: val_acc improved from 0.11234 to 0.12768, saving model to drive/My Drive/weights_folder/weights_best_new2.h5\n",
            "Epoch 10/50\n",
            "200/200 [==============================] - 402s 2s/step - loss: 6.3576 - acc: 0.1734 - val_loss: 6.4870 - val_acc: 0.1478\n",
            "\n",
            "Epoch 00010: val_acc improved from 0.12768 to 0.14783, saving model to drive/My Drive/weights_folder/weights_best_new2.h5\n",
            "Epoch 11/50\n",
            "200/200 [==============================] - 402s 2s/step - loss: 6.2723 - acc: 0.1815 - val_loss: 6.4010 - val_acc: 0.1584\n",
            "\n",
            "Epoch 00011: val_acc improved from 0.14783 to 0.15844, saving model to drive/My Drive/weights_folder/weights_best_new2.h5\n",
            "Epoch 12/50\n",
            "200/200 [==============================] - 402s 2s/step - loss: 6.1437 - acc: 0.2009 - val_loss: 6.3344 - val_acc: 0.1659\n",
            "\n",
            "Epoch 00012: val_acc improved from 0.15844 to 0.16593, saving model to drive/My Drive/weights_folder/weights_best_new2.h5\n",
            "Epoch 13/50\n",
            "200/200 [==============================] - 402s 2s/step - loss: 6.0645 - acc: 0.2121 - val_loss: 6.2581 - val_acc: 0.1741\n",
            "\n",
            "Epoch 00013: val_acc improved from 0.16593 to 0.17406, saving model to drive/My Drive/weights_folder/weights_best_new2.h5\n",
            "Epoch 14/50\n",
            "200/200 [==============================] - 402s 2s/step - loss: 5.9818 - acc: 0.2204 - val_loss: 6.2815 - val_acc: 0.1678\n",
            "\n",
            "Epoch 00014: val_acc did not improve from 0.17406\n",
            "Epoch 15/50\n",
            "200/200 [==============================] - 402s 2s/step - loss: 5.9237 - acc: 0.2311 - val_loss: 6.1227 - val_acc: 0.1869\n",
            "\n",
            "Epoch 00015: val_acc improved from 0.17406 to 0.18687, saving model to drive/My Drive/weights_folder/weights_best_new2.h5\n",
            "Epoch 16/50\n",
            "200/200 [==============================] - 402s 2s/step - loss: 5.8436 - acc: 0.2428 - val_loss: 6.1399 - val_acc: 0.1855\n",
            "\n",
            "Epoch 00016: val_acc did not improve from 0.18687\n",
            "Epoch 17/50\n",
            "200/200 [==============================] - 402s 2s/step - loss: 5.8232 - acc: 0.2449 - val_loss: 6.0488 - val_acc: 0.2010\n",
            "\n",
            "Epoch 00017: val_acc improved from 0.18687 to 0.20104, saving model to drive/My Drive/weights_folder/weights_best_new2.h5\n",
            "Epoch 18/50\n",
            "200/200 [==============================] - 402s 2s/step - loss: 5.7663 - acc: 0.2534 - val_loss: 6.0678 - val_acc: 0.1957\n",
            "\n",
            "Epoch 00018: val_acc did not improve from 0.20104\n",
            "Epoch 19/50\n",
            "200/200 [==============================] - 403s 2s/step - loss: 5.7260 - acc: 0.2612 - val_loss: 6.0379 - val_acc: 0.2053\n",
            "\n",
            "Epoch 00019: val_acc improved from 0.20104 to 0.20531, saving model to drive/My Drive/weights_folder/weights_best_new2.h5\n",
            "Epoch 20/50\n",
            "200/200 [==============================] - 402s 2s/step - loss: 5.7197 - acc: 0.2594 - val_loss: 6.0310 - val_acc: 0.2018\n",
            "\n",
            "Epoch 00020: val_acc did not improve from 0.20531\n",
            "Epoch 21/50\n",
            "200/200 [==============================] - 402s 2s/step - loss: 5.6859 - acc: 0.2679 - val_loss: 6.0277 - val_acc: 0.2018\n",
            "\n",
            "Epoch 00021: val_acc did not improve from 0.20531\n",
            "Epoch 22/50\n",
            "200/200 [==============================] - 402s 2s/step - loss: 5.6783 - acc: 0.2682 - val_loss: 6.0149 - val_acc: 0.2072\n",
            "\n",
            "Epoch 00022: val_acc improved from 0.20531 to 0.20719, saving model to drive/My Drive/weights_folder/weights_best_new2.h5\n",
            "Epoch 23/50\n",
            "200/200 [==============================] - 403s 2s/step - loss: 5.6758 - acc: 0.2669 - val_loss: 5.9768 - val_acc: 0.2092\n",
            "\n",
            "Epoch 00023: val_acc improved from 0.20719 to 0.20923, saving model to drive/My Drive/weights_folder/weights_best_new2.h5\n",
            "Epoch 24/50\n",
            "200/200 [==============================] - 403s 2s/step - loss: 5.6513 - acc: 0.2696 - val_loss: 5.9686 - val_acc: 0.2092\n",
            "\n",
            "Epoch 00024: val_acc did not improve from 0.20923\n",
            "Epoch 25/50\n",
            "200/200 [==============================] - 402s 2s/step - loss: 5.6220 - acc: 0.2729 - val_loss: 5.9449 - val_acc: 0.2102\n",
            "\n",
            "Epoch 00025: val_acc improved from 0.20923 to 0.21017, saving model to drive/My Drive/weights_folder/weights_best_new2.h5\n",
            "Epoch 26/50\n",
            "200/200 [==============================] - 405s 2s/step - loss: 5.5990 - acc: 0.2726 - val_loss: 5.9169 - val_acc: 0.2171\n",
            "\n",
            "Epoch 00026: val_acc improved from 0.21017 to 0.21710, saving model to drive/My Drive/weights_folder/weights_best_new2.h5\n",
            "Epoch 27/50\n",
            "200/200 [==============================] - 404s 2s/step - loss: 5.5724 - acc: 0.2784 - val_loss: 5.8586 - val_acc: 0.2217\n",
            "\n",
            "Epoch 00027: val_acc improved from 0.21710 to 0.22172, saving model to drive/My Drive/weights_folder/weights_best_new2.h5\n",
            "Epoch 28/50\n",
            "200/200 [==============================] - 403s 2s/step - loss: 5.5004 - acc: 0.2885 - val_loss: 5.8505 - val_acc: 0.2179\n",
            "\n",
            "Epoch 00028: val_acc did not improve from 0.22172\n",
            "Epoch 29/50\n",
            "200/200 [==============================] - 404s 2s/step - loss: 5.4966 - acc: 0.2871 - val_loss: 5.8100 - val_acc: 0.2339\n",
            "\n",
            "Epoch 00029: val_acc improved from 0.22172 to 0.23394, saving model to drive/My Drive/weights_folder/weights_best_new2.h5\n",
            "Epoch 30/50\n",
            "200/200 [==============================] - 403s 2s/step - loss: 5.4322 - acc: 0.2935 - val_loss: 5.7529 - val_acc: 0.2367\n",
            "\n",
            "Epoch 00030: val_acc improved from 0.23394 to 0.23672, saving model to drive/My Drive/weights_folder/weights_best_new2.h5\n",
            "Epoch 31/50\n",
            "200/200 [==============================] - 403s 2s/step - loss: 5.3883 - acc: 0.3014 - val_loss: 5.6980 - val_acc: 0.2388\n",
            "\n",
            "Epoch 00031: val_acc improved from 0.23672 to 0.23882, saving model to drive/My Drive/weights_folder/weights_best_new2.h5\n",
            "Epoch 32/50\n",
            "200/200 [==============================] - 403s 2s/step - loss: 5.3110 - acc: 0.3103 - val_loss: 5.6687 - val_acc: 0.2431\n",
            "\n",
            "Epoch 00032: val_acc improved from 0.23882 to 0.24307, saving model to drive/My Drive/weights_folder/weights_best_new2.h5\n",
            "Epoch 33/50\n",
            "200/200 [==============================] - 403s 2s/step - loss: 5.2626 - acc: 0.3193 - val_loss: 5.6097 - val_acc: 0.2566\n",
            "\n",
            "Epoch 00033: val_acc improved from 0.24307 to 0.25656, saving model to drive/My Drive/weights_folder/weights_best_new2.h5\n",
            "Epoch 34/50\n",
            "200/200 [==============================] - 403s 2s/step - loss: 5.1889 - acc: 0.3286 - val_loss: 5.5643 - val_acc: 0.2572\n",
            "\n",
            "Epoch 00034: val_acc improved from 0.25656 to 0.25724, saving model to drive/My Drive/weights_folder/weights_best_new2.h5\n",
            "Epoch 35/50\n",
            "200/200 [==============================] - 403s 2s/step - loss: 5.1634 - acc: 0.3322 - val_loss: 5.5296 - val_acc: 0.2634\n",
            "\n",
            "Epoch 00035: val_acc improved from 0.25724 to 0.26344, saving model to drive/My Drive/weights_folder/weights_best_new2.h5\n",
            "Epoch 36/50\n",
            "200/200 [==============================] - 403s 2s/step - loss: 5.1018 - acc: 0.3424 - val_loss: 5.5026 - val_acc: 0.2594\n",
            "\n",
            "Epoch 00036: val_acc did not improve from 0.26344\n",
            "Epoch 37/50\n",
            "200/200 [==============================] - 402s 2s/step - loss: 5.0738 - acc: 0.3468 - val_loss: 5.4821 - val_acc: 0.2667\n",
            "\n",
            "Epoch 00037: val_acc improved from 0.26344 to 0.26669, saving model to drive/My Drive/weights_folder/weights_best_new2.h5\n",
            "Epoch 38/50\n",
            "200/200 [==============================] - 403s 2s/step - loss: 5.0280 - acc: 0.3551 - val_loss: 5.4705 - val_acc: 0.2700\n",
            "\n",
            "Epoch 00038: val_acc improved from 0.26669 to 0.27000, saving model to drive/My Drive/weights_folder/weights_best_new2.h5\n",
            "Epoch 39/50\n",
            "200/200 [==============================] - 403s 2s/step - loss: 5.0074 - acc: 0.3589 - val_loss: 5.4552 - val_acc: 0.2746\n",
            "\n",
            "Epoch 00039: val_acc improved from 0.27000 to 0.27456, saving model to drive/My Drive/weights_folder/weights_best_new2.h5\n",
            "Epoch 40/50\n",
            "200/200 [==============================] - 403s 2s/step - loss: 4.9690 - acc: 0.3658 - val_loss: 5.4066 - val_acc: 0.2739\n",
            "\n",
            "Epoch 00040: val_acc did not improve from 0.27456\n",
            "Epoch 41/50\n",
            "200/200 [==============================] - 403s 2s/step - loss: 4.9657 - acc: 0.3659 - val_loss: 5.4430 - val_acc: 0.2764\n",
            "\n",
            "Epoch 00041: val_acc improved from 0.27456 to 0.27641, saving model to drive/My Drive/weights_folder/weights_best_new2.h5\n",
            "Epoch 42/50\n",
            "200/200 [==============================] - 403s 2s/step - loss: 4.9595 - acc: 0.3653 - val_loss: 5.4118 - val_acc: 0.2787\n",
            "\n",
            "Epoch 00042: val_acc improved from 0.27641 to 0.27865, saving model to drive/My Drive/weights_folder/weights_best_new2.h5\n",
            "Epoch 43/50\n",
            "200/200 [==============================] - 403s 2s/step - loss: 4.9654 - acc: 0.3606 - val_loss: 5.4150 - val_acc: 0.2785\n",
            "\n",
            "Epoch 00043: val_acc did not improve from 0.27865\n",
            "Epoch 44/50\n",
            "200/200 [==============================] - 403s 2s/step - loss: 4.9469 - acc: 0.3643 - val_loss: 5.3888 - val_acc: 0.2784\n",
            "\n",
            "Epoch 00044: val_acc did not improve from 0.27865\n",
            "Epoch 45/50\n",
            "200/200 [==============================] - 402s 2s/step - loss: 4.9543 - acc: 0.3631 - val_loss: 5.4081 - val_acc: 0.2757\n",
            "\n",
            "Epoch 00045: val_acc did not improve from 0.27865\n",
            "Epoch 46/50\n",
            "200/200 [==============================] - 402s 2s/step - loss: 4.9240 - acc: 0.3650 - val_loss: 5.3899 - val_acc: 0.2805\n",
            "\n",
            "Epoch 00046: val_acc improved from 0.27865 to 0.28047, saving model to drive/My Drive/weights_folder/weights_best_new2.h5\n",
            "Epoch 47/50\n",
            "200/200 [==============================] - 402s 2s/step - loss: 4.9293 - acc: 0.3635 - val_loss: 5.3627 - val_acc: 0.2805\n",
            "\n",
            "Epoch 00047: val_acc improved from 0.28047 to 0.28054, saving model to drive/My Drive/weights_folder/weights_best_new2.h5\n",
            "Epoch 48/50\n",
            "200/200 [==============================] - 400s 2s/step - loss: 4.8856 - acc: 0.3690 - val_loss: 5.3331 - val_acc: 0.2831\n",
            "\n",
            "Epoch 00048: val_acc improved from 0.28054 to 0.28306, saving model to drive/My Drive/weights_folder/weights_best_new2.h5\n",
            "Epoch 49/50\n",
            "200/200 [==============================] - 400s 2s/step - loss: 4.8982 - acc: 0.3621 - val_loss: 5.3421 - val_acc: 0.2748\n",
            "\n",
            "Epoch 00049: val_acc did not improve from 0.28306\n",
            "Epoch 50/50\n",
            "200/200 [==============================] - 401s 2s/step - loss: 4.8286 - acc: 0.3731 - val_loss: 5.2674 - val_acc: 0.2898\n",
            "\n",
            "Epoch 00050: val_acc improved from 0.28306 to 0.28983, saving model to drive/My Drive/weights_folder/weights_best_new2.h5\n",
            "Epoch 1/50\n",
            "200/200 [==============================] - 402s 2s/step - loss: 4.8183 - acc: 0.3747 - val_loss: 5.2546 - val_acc: 0.2880\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.28983\n",
            "Epoch 2/50\n",
            "200/200 [==============================] - 400s 2s/step - loss: 4.7565 - acc: 0.3842 - val_loss: 5.2000 - val_acc: 0.2977\n",
            "\n",
            "Epoch 00002: val_acc improved from 0.28983 to 0.29770, saving model to drive/My Drive/weights_folder/weights_best_new2.h5\n",
            "Epoch 3/50\n",
            "200/200 [==============================] - 399s 2s/step - loss: 4.7056 - acc: 0.3895 - val_loss: 5.1911 - val_acc: 0.2944\n",
            "\n",
            "Epoch 00003: val_acc did not improve from 0.29770\n",
            "Epoch 4/50\n",
            "200/200 [==============================] - 400s 2s/step - loss: 4.6598 - acc: 0.3978 - val_loss: 5.1543 - val_acc: 0.3030\n",
            "\n",
            "Epoch 00004: val_acc improved from 0.29770 to 0.30297, saving model to drive/My Drive/weights_folder/weights_best_new2.h5\n",
            "Epoch 5/50\n",
            "200/200 [==============================] - 399s 2s/step - loss: 4.6238 - acc: 0.4013 - val_loss: 5.1457 - val_acc: 0.2983\n",
            "\n",
            "Epoch 00005: val_acc did not improve from 0.30297\n",
            "Epoch 6/50\n",
            "200/200 [==============================] - 399s 2s/step - loss: 4.5712 - acc: 0.4126 - val_loss: 5.0799 - val_acc: 0.3075\n",
            "\n",
            "Epoch 00006: val_acc improved from 0.30297 to 0.30750, saving model to drive/My Drive/weights_folder/weights_best_new2.h5\n",
            "Epoch 7/50\n",
            "200/200 [==============================] - 399s 2s/step - loss: 4.5384 - acc: 0.4183 - val_loss: 5.0918 - val_acc: 0.3101\n",
            "\n",
            "Epoch 00007: val_acc improved from 0.30750 to 0.31014, saving model to drive/My Drive/weights_folder/weights_best_new2.h5\n",
            "Epoch 8/50\n",
            "200/200 [==============================] - 399s 2s/step - loss: 4.5062 - acc: 0.4234 - val_loss: 5.0649 - val_acc: 0.3081\n",
            "\n",
            "Epoch 00008: val_acc did not improve from 0.31014\n",
            "Epoch 9/50\n",
            "200/200 [==============================] - 400s 2s/step - loss: 4.4776 - acc: 0.4270 - val_loss: 5.0430 - val_acc: 0.3192\n",
            "\n",
            "Epoch 00009: val_acc improved from 0.31014 to 0.31922, saving model to drive/My Drive/weights_folder/weights_best_new2.h5\n",
            "Epoch 10/50\n",
            "200/200 [==============================] - 399s 2s/step - loss: 4.4445 - acc: 0.4361 - val_loss: 5.0361 - val_acc: 0.3194\n",
            "\n",
            "Epoch 00010: val_acc improved from 0.31922 to 0.31943, saving model to drive/My Drive/weights_folder/weights_best_new2.h5\n",
            "Epoch 11/50\n",
            "200/200 [==============================] - 399s 2s/step - loss: 4.4392 - acc: 0.4364 - val_loss: 5.0530 - val_acc: 0.3103\n",
            "\n",
            "Epoch 00011: val_acc did not improve from 0.31943\n",
            "Epoch 12/50\n",
            "200/200 [==============================] - 401s 2s/step - loss: 4.4461 - acc: 0.4318 - val_loss: 5.0517 - val_acc: 0.3142\n",
            "\n",
            "Epoch 00012: val_acc did not improve from 0.31943\n",
            "Epoch 13/50\n",
            "200/200 [==============================] - 402s 2s/step - loss: 4.4497 - acc: 0.4324 - val_loss: 5.0311 - val_acc: 0.3117\n",
            "\n",
            "Epoch 00013: val_acc did not improve from 0.31943\n",
            "Epoch 14/50\n",
            "200/200 [==============================] - 402s 2s/step - loss: 4.4516 - acc: 0.4288 - val_loss: 5.0263 - val_acc: 0.3218\n",
            "\n",
            "Epoch 00014: val_acc improved from 0.31943 to 0.32179, saving model to drive/My Drive/weights_folder/weights_best_new2.h5\n",
            "Epoch 15/50\n",
            "200/200 [==============================] - 402s 2s/step - loss: 4.4455 - acc: 0.4283 - val_loss: 5.0113 - val_acc: 0.3227\n",
            "\n",
            "Epoch 00015: val_acc improved from 0.32179 to 0.32266, saving model to drive/My Drive/weights_folder/weights_best_new2.h5\n",
            "Epoch 16/50\n",
            "200/200 [==============================] - 402s 2s/step - loss: 4.4462 - acc: 0.4283 - val_loss: 4.9969 - val_acc: 0.3101\n",
            "\n",
            "Epoch 00016: val_acc did not improve from 0.32266\n",
            "Epoch 17/50\n",
            "200/200 [==============================] - 402s 2s/step - loss: 4.4304 - acc: 0.4288 - val_loss: 4.9900 - val_acc: 0.3206\n",
            "\n",
            "Epoch 00017: val_acc did not improve from 0.32266\n",
            "Epoch 18/50\n",
            "200/200 [==============================] - 402s 2s/step - loss: 4.4251 - acc: 0.4268 - val_loss: 4.9866 - val_acc: 0.3149\n",
            "\n",
            "Epoch 00018: val_acc did not improve from 0.32266\n",
            "Epoch 19/50\n",
            "200/200 [==============================] - 402s 2s/step - loss: 4.4223 - acc: 0.4257 - val_loss: 4.9910 - val_acc: 0.3094\n",
            "\n",
            "Epoch 00019: val_acc did not improve from 0.32266\n",
            "Epoch 20/50\n",
            "200/200 [==============================] - 402s 2s/step - loss: 4.4033 - acc: 0.4269 - val_loss: 4.9645 - val_acc: 0.3214\n",
            "\n",
            "Epoch 00020: val_acc did not improve from 0.32266\n",
            "Epoch 21/50\n",
            "200/200 [==============================] - 402s 2s/step - loss: 4.3738 - acc: 0.4312 - val_loss: 4.9139 - val_acc: 0.3237\n",
            "\n",
            "Epoch 00021: val_acc improved from 0.32266 to 0.32368, saving model to drive/My Drive/weights_folder/weights_best_new2.h5\n",
            "Epoch 22/50\n",
            "200/200 [==============================] - 402s 2s/step - loss: 4.3244 - acc: 0.4383 - val_loss: 4.9205 - val_acc: 0.3190\n",
            "\n",
            "Epoch 00022: val_acc did not improve from 0.32368\n",
            "Epoch 23/50\n",
            "200/200 [==============================] - 402s 2s/step - loss: 4.2759 - acc: 0.4456 - val_loss: 4.8934 - val_acc: 0.3241\n",
            "\n",
            "Epoch 00023: val_acc improved from 0.32368 to 0.32406, saving model to drive/My Drive/weights_folder/weights_best_new2.h5\n",
            "Epoch 24/50\n",
            "200/200 [==============================] - 402s 2s/step - loss: 4.2432 - acc: 0.4506 - val_loss: 4.8197 - val_acc: 0.3424\n",
            "\n",
            "Epoch 00024: val_acc improved from 0.32406 to 0.34241, saving model to drive/My Drive/weights_folder/weights_best_new2.h5\n",
            "Epoch 25/50\n",
            "200/200 [==============================] - 402s 2s/step - loss: 4.2004 - acc: 0.4560 - val_loss: 4.8081 - val_acc: 0.3341\n",
            "\n",
            "Epoch 00025: val_acc did not improve from 0.34241\n",
            "Epoch 26/50\n",
            "200/200 [==============================] - 402s 2s/step - loss: 4.1647 - acc: 0.4646 - val_loss: 4.8053 - val_acc: 0.3459\n",
            "\n",
            "Epoch 00026: val_acc improved from 0.34241 to 0.34594, saving model to drive/My Drive/weights_folder/weights_best_new2.h5\n",
            "Epoch 27/50\n",
            "200/200 [==============================] - 402s 2s/step - loss: 4.1059 - acc: 0.4759 - val_loss: 4.7661 - val_acc: 0.3407\n",
            "\n",
            "Epoch 00027: val_acc did not improve from 0.34594\n",
            "Epoch 28/50\n",
            "200/200 [==============================] - 402s 2s/step - loss: 4.0869 - acc: 0.4806 - val_loss: 4.7808 - val_acc: 0.3366\n",
            "\n",
            "Epoch 00028: val_acc did not improve from 0.34594\n",
            "Epoch 29/50\n",
            "200/200 [==============================] - 402s 2s/step - loss: 4.0596 - acc: 0.4836 - val_loss: 4.7272 - val_acc: 0.3514\n",
            "\n",
            "Epoch 00029: val_acc improved from 0.34594 to 0.35139, saving model to drive/My Drive/weights_folder/weights_best_new2.h5\n",
            "Epoch 30/50\n",
            "200/200 [==============================] - 402s 2s/step - loss: 4.0287 - acc: 0.4939 - val_loss: 4.7251 - val_acc: 0.3548\n",
            "\n",
            "Epoch 00030: val_acc improved from 0.35139 to 0.35485, saving model to drive/My Drive/weights_folder/weights_best_new2.h5\n",
            "Epoch 31/50\n",
            "200/200 [==============================] - 402s 2s/step - loss: 4.0042 - acc: 0.4953 - val_loss: 4.7246 - val_acc: 0.3503\n",
            "\n",
            "Epoch 00031: val_acc did not improve from 0.35485\n",
            "Epoch 32/50\n",
            "200/200 [==============================] - 402s 2s/step - loss: 4.0242 - acc: 0.4922 - val_loss: 4.7657 - val_acc: 0.3460\n",
            "\n",
            "Epoch 00032: val_acc did not improve from 0.35485\n",
            "Epoch 33/50\n",
            "200/200 [==============================] - 402s 2s/step - loss: 4.0305 - acc: 0.4888 - val_loss: 4.7423 - val_acc: 0.3441\n",
            "\n",
            "Epoch 00033: val_acc did not improve from 0.35485\n",
            "Epoch 34/50\n",
            "200/200 [==============================] - 402s 2s/step - loss: 4.0392 - acc: 0.4840 - val_loss: 4.7622 - val_acc: 0.3469\n",
            "\n",
            "Epoch 00034: val_acc did not improve from 0.35485\n",
            "Epoch 35/50\n",
            "200/200 [==============================] - 402s 2s/step - loss: 4.0314 - acc: 0.4835 - val_loss: 4.7367 - val_acc: 0.3519\n",
            "\n",
            "Epoch 00035: val_acc did not improve from 0.35485\n",
            "Epoch 36/50\n",
            "200/200 [==============================] - 402s 2s/step - loss: 4.0425 - acc: 0.4816 - val_loss: 4.7476 - val_acc: 0.3429\n",
            "\n",
            "Epoch 00036: val_acc did not improve from 0.35485\n",
            "Epoch 37/50\n",
            "200/200 [==============================] - 402s 2s/step - loss: 4.0433 - acc: 0.4811 - val_loss: 4.7767 - val_acc: 0.3278\n",
            "\n",
            "Epoch 00037: val_acc did not improve from 0.35485\n",
            "Epoch 38/50\n",
            "200/200 [==============================] - 402s 2s/step - loss: 4.0553 - acc: 0.4739 - val_loss: 4.7272 - val_acc: 0.3388\n",
            "\n",
            "Epoch 00038: val_acc did not improve from 0.35485\n",
            "Epoch 39/50\n",
            "200/200 [==============================] - 402s 2s/step - loss: 4.0366 - acc: 0.4766 - val_loss: 4.7220 - val_acc: 0.3350\n",
            "\n",
            "Epoch 00039: val_acc did not improve from 0.35485\n",
            "Epoch 40/50\n",
            "200/200 [==============================] - 402s 2s/step - loss: 4.0289 - acc: 0.4724 - val_loss: 4.7008 - val_acc: 0.3416\n",
            "\n",
            "Epoch 00040: val_acc did not improve from 0.35485\n",
            "Epoch 41/50\n",
            "200/200 [==============================] - 402s 2s/step - loss: 4.0246 - acc: 0.4741 - val_loss: 4.6669 - val_acc: 0.3452\n",
            "\n",
            "Epoch 00041: val_acc did not improve from 0.35485\n",
            "Epoch 42/50\n",
            "200/200 [==============================] - 402s 2s/step - loss: 3.9656 - acc: 0.4835 - val_loss: 4.6610 - val_acc: 0.3409\n",
            "\n",
            "Epoch 00042: val_acc did not improve from 0.35485\n",
            "Epoch 43/50\n",
            "200/200 [==============================] - 405s 2s/step - loss: 3.9224 - acc: 0.4900 - val_loss: 4.6129 - val_acc: 0.3569\n",
            "\n",
            "Epoch 00043: val_acc improved from 0.35485 to 0.35690, saving model to drive/My Drive/weights_folder/weights_best_new2.h5\n",
            "Epoch 44/50\n",
            "200/200 [==============================] - 405s 2s/step - loss: 3.8851 - acc: 0.4997 - val_loss: 4.5999 - val_acc: 0.3560\n",
            "\n",
            "Epoch 00044: val_acc did not improve from 0.35690\n",
            "Epoch 45/50\n",
            "200/200 [==============================] - 405s 2s/step - loss: 3.8406 - acc: 0.5042 - val_loss: 4.5563 - val_acc: 0.3644\n",
            "\n",
            "Epoch 00045: val_acc improved from 0.35690 to 0.36438, saving model to drive/My Drive/weights_folder/weights_best_new2.h5\n",
            "Epoch 46/50\n",
            "200/200 [==============================] - 405s 2s/step - loss: 3.8034 - acc: 0.5122 - val_loss: 4.5822 - val_acc: 0.3578\n",
            "\n",
            "Epoch 00046: val_acc did not improve from 0.36438\n",
            "Epoch 47/50\n",
            "200/200 [==============================] - 405s 2s/step - loss: 3.7483 - acc: 0.5242 - val_loss: 4.5813 - val_acc: 0.3615\n",
            "\n",
            "Epoch 00047: val_acc did not improve from 0.36438\n",
            "Epoch 48/50\n",
            "200/200 [==============================] - 406s 2s/step - loss: 3.7189 - acc: 0.5287 - val_loss: 4.5117 - val_acc: 0.3655\n",
            "\n",
            "Epoch 00048: val_acc improved from 0.36438 to 0.36547, saving model to drive/My Drive/weights_folder/weights_best_new2.h5\n",
            "Epoch 49/50\n",
            "200/200 [==============================] - 405s 2s/step - loss: 3.7061 - acc: 0.5304 - val_loss: 4.5034 - val_acc: 0.3756\n",
            "\n",
            "Epoch 00049: val_acc improved from 0.36547 to 0.37563, saving model to drive/My Drive/weights_folder/weights_best_new2.h5\n",
            "Epoch 50/50\n",
            "200/200 [==============================] - 405s 2s/step - loss: 3.6642 - acc: 0.5400 - val_loss: 4.5314 - val_acc: 0.3653\n",
            "\n",
            "Epoch 00050: val_acc did not improve from 0.37563\n",
            "Saving model to disk\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JfcjmzHHDcjh",
        "colab_type": "code",
        "outputId": "29f8ba7e-ebc0-4117-f5ac-da6dc5aad752",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1328
        }
      },
      "source": [
        "from keras.callbacks import ReduceLROnPlateau, CSVLogger, EarlyStopping,ModelCheckpoint\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.models import load_model\n",
        "data_augmentation = True\n",
        "batch_size = 128\n",
        "epoch = 1\n",
        "from keras.utils import np_utils\n",
        "#call_backs\n",
        "lr_reducer = ReduceLROnPlateau(factor=np.sqrt(0.1), cooldown=0, patience=5, min_lr=0.5e-6)\n",
        "early_stopper = EarlyStopping(min_delta=0.001, patience=5)\n",
        "filepath=\"drive/My Drive/weights_folder/weights_best_new2.h5\"\n",
        "checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
        "callbacks_list = [checkpoint]\n",
        "adm=SGD(lr=0.001, decay=1e-5, momentum=0.95, nesterov=True)\n",
        "model.compile(loss='categorical_crossentropy', metrics=['accuracy'],optimizer=adm)\n",
        "\n",
        "\n",
        "if not data_augmentation:\n",
        "    print('Not using data augmentation.')\n",
        "    model.fit(X_train, Y_train,\n",
        "              batch_size=batch_size,\n",
        "              epochs=epoch,\n",
        "              validation_data=(X_test, Y_test),\n",
        "              shuffle=True,\n",
        "              callbacks=callbacks_list)\n",
        "else:\n",
        "    print('Using real-time data augmentation.')\n",
        "    # This will do preprocessing and realtime data augmentation:\n",
        "    datagen = ImageDataGenerator(\n",
        "        featurewise_center=True,  # set input mean to 0 over the dataset\n",
        "        samplewise_center=True,  # set each sample mean to 0\n",
        "        featurewise_std_normalization=False,  # divide inputs by std of the dataset\n",
        "        samplewise_std_normalization=False,  # divide each input by its std\n",
        "        zca_whitening=False,  # apply ZCA whitening\n",
        "        rotation_range=45,  # randomly rotate images in the range (degrees, 0 to 180)\n",
        "        width_shift_range=0.1,  # randomly shift images horizontally (fraction of total width)\n",
        "        height_shift_range=0.1,  # randomly shift images vertically (fraction of total height)\n",
        "        horizontal_flip=True,  # randomly flip images\n",
        "        vertical_flip=False)  # randomly flip images\n",
        "\n",
        "    for i in range(epoch):\n",
        "      print(\"Loading Latest model weights \")\n",
        "      model = load_model(filepath)\n",
        "      model.fit_generator(train_generator,\n",
        "                          steps_per_epoch=200,\n",
        "                          validation_data=validation_generator, validation_steps=100,\n",
        "                          epochs=15, verbose=1, max_q_size=100,callbacks=callbacks_list)\n",
        "    print(\"Saving model to disk\")\n",
        "    model.save(filepath)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using real-time data augmentation.\n",
            "Loading Latest model weights \n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:47: UserWarning: Update your `fit_generator` call to the Keras 2 API: `fit_generator(<keras_pre..., steps_per_epoch=200, validation_data=<keras_pre..., validation_steps=100, epochs=15, verbose=1, callbacks=[<keras.ca..., max_queue_size=100)`\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/15\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/keras_preprocessing/image/image_data_generator.py:699: UserWarning: This ImageDataGenerator specifies `featurewise_center`, but it hasn't been fit on any training data. Fit it first by calling `.fit(numpy_data)`.\n",
            "  warnings.warn('This ImageDataGenerator specifies '\n",
            "/usr/local/lib/python3.6/dist-packages/keras_preprocessing/image/image_data_generator.py:707: UserWarning: This ImageDataGenerator specifies `featurewise_std_normalization`, but it hasn't been fit on any training data. Fit it first by calling `.fit(numpy_data)`.\n",
            "  warnings.warn('This ImageDataGenerator specifies '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "200/200 [==============================] - 411s 2s/step - loss: 3.4853 - acc: 0.5798 - val_loss: 4.4716 - val_acc: 0.3752\n",
            "\n",
            "Epoch 00001: val_acc improved from -inf to 0.37516, saving model to drive/My Drive/weights_folder/weights_best_new2.h5\n",
            "Epoch 2/15\n",
            "200/200 [==============================] - 390s 2s/step - loss: 3.4897 - acc: 0.5785 - val_loss: 4.4638 - val_acc: 0.3730\n",
            "\n",
            "Epoch 00002: val_acc did not improve from 0.37516\n",
            "Epoch 3/15\n",
            "200/200 [==============================] - 390s 2s/step - loss: 3.4703 - acc: 0.5824 - val_loss: 4.4922 - val_acc: 0.3681\n",
            "\n",
            "Epoch 00003: val_acc did not improve from 0.37516\n",
            "Epoch 4/15\n",
            "200/200 [==============================] - 389s 2s/step - loss: 3.4827 - acc: 0.5791 - val_loss: 4.4886 - val_acc: 0.3693\n",
            "\n",
            "Epoch 00004: val_acc did not improve from 0.37516\n",
            "Epoch 5/15\n",
            "200/200 [==============================] - 390s 2s/step - loss: 3.4597 - acc: 0.5847 - val_loss: 4.4960 - val_acc: 0.3759\n",
            "\n",
            "Epoch 00005: val_acc improved from 0.37516 to 0.37594, saving model to drive/My Drive/weights_folder/weights_best_new2.h5\n",
            "Epoch 6/15\n",
            "200/200 [==============================] - 390s 2s/step - loss: 3.4681 - acc: 0.5841 - val_loss: 4.4559 - val_acc: 0.3748\n",
            "\n",
            "Epoch 00006: val_acc did not improve from 0.37594\n",
            "Epoch 7/15\n",
            "200/200 [==============================] - 390s 2s/step - loss: 3.4539 - acc: 0.5825 - val_loss: 4.4671 - val_acc: 0.3778\n",
            "\n",
            "Epoch 00007: val_acc improved from 0.37594 to 0.37783, saving model to drive/My Drive/weights_folder/weights_best_new2.h5\n",
            "Epoch 8/15\n",
            "200/200 [==============================] - 390s 2s/step - loss: 3.4533 - acc: 0.5847 - val_loss: 4.4686 - val_acc: 0.3775\n",
            "\n",
            "Epoch 00008: val_acc did not improve from 0.37783\n",
            "Epoch 9/15\n",
            "200/200 [==============================] - 389s 2s/step - loss: 3.4375 - acc: 0.5865 - val_loss: 4.4865 - val_acc: 0.3609\n",
            "\n",
            "Epoch 00009: val_acc did not improve from 0.37783\n",
            "Epoch 10/15\n",
            "200/200 [==============================] - 390s 2s/step - loss: 3.4559 - acc: 0.5820 - val_loss: 4.4731 - val_acc: 0.3676\n",
            "\n",
            "Epoch 00010: val_acc did not improve from 0.37783\n",
            "Epoch 11/15\n",
            "200/200 [==============================] - 389s 2s/step - loss: 3.4273 - acc: 0.5880 - val_loss: 4.4514 - val_acc: 0.3785\n",
            "\n",
            "Epoch 00011: val_acc improved from 0.37783 to 0.37846, saving model to drive/My Drive/weights_folder/weights_best_new2.h5\n",
            "Epoch 12/15\n",
            "200/200 [==============================] - 390s 2s/step - loss: 3.4403 - acc: 0.5862 - val_loss: 4.4451 - val_acc: 0.3742\n",
            "\n",
            "Epoch 00012: val_acc did not improve from 0.37846\n",
            "Epoch 13/15\n",
            "200/200 [==============================] - 390s 2s/step - loss: 3.4164 - acc: 0.5900 - val_loss: 4.4891 - val_acc: 0.3748\n",
            "\n",
            "Epoch 00013: val_acc did not improve from 0.37846\n",
            "Epoch 14/15\n",
            "200/200 [==============================] - 390s 2s/step - loss: 3.4204 - acc: 0.5894 - val_loss: 4.4478 - val_acc: 0.3839\n",
            "\n",
            "Epoch 00014: val_acc improved from 0.37846 to 0.38391, saving model to drive/My Drive/weights_folder/weights_best_new2.h5\n",
            "Epoch 15/15\n",
            "200/200 [==============================] - 390s 2s/step - loss: 3.4061 - acc: 0.5940 - val_loss: 4.4914 - val_acc: 0.3673\n",
            "\n",
            "Epoch 00015: val_acc did not improve from 0.38391\n",
            "Saving model to disk\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vk6VH6lsUO26",
        "colab_type": "code",
        "outputId": "f026eaf2-321d-44af-b163-86f13cae15ea",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 3783
        }
      },
      "source": [
        "from keras.callbacks import ReduceLROnPlateau, CSVLogger, EarlyStopping,ModelCheckpoint\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.models import load_model\n",
        "data_augmentation = True\n",
        "batch_size = 128\n",
        "epoch = 1\n",
        "from keras.utils import np_utils\n",
        "#call_backs\n",
        "lr_reducer = ReduceLROnPlateau(factor=np.sqrt(0.1), cooldown=0, patience=5, min_lr=0.5e-6)\n",
        "early_stopper = EarlyStopping(min_delta=0.001, patience=5)\n",
        "filepath=\"drive/My Drive/weights_folder/weights_best_new2.h5\"\n",
        "checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
        "callbacks_list = [checkpoint,clr_triangular,lr_reducer]\n",
        "adm=SGD(lr=0.001, decay=1e-5, momentum=0.95, nesterov=True)\n",
        "model.compile(loss='categorical_crossentropy', metrics=['accuracy'],optimizer=adm)\n",
        "\n",
        "\n",
        "if not data_augmentation:\n",
        "    print('Not using data augmentation.')\n",
        "    model.fit(X_train, Y_train,\n",
        "              batch_size=batch_size,\n",
        "              epochs=epoch,\n",
        "              validation_data=(X_test, Y_test),\n",
        "              shuffle=True,\n",
        "              callbacks=callbacks_list)\n",
        "else:\n",
        "    print('Using real-time data augmentation.')\n",
        "    # This will do preprocessing and realtime data augmentation:\n",
        "    datagen = ImageDataGenerator(\n",
        "        featurewise_center=True,  # set input mean to 0 over the dataset\n",
        "        samplewise_center=True,  # set each sample mean to 0\n",
        "        featurewise_std_normalization=False,  # divide inputs by std of the dataset\n",
        "        samplewise_std_normalization=False,  # divide each input by its std\n",
        "        zca_whitening=False,  # apply ZCA whitening\n",
        "        rotation_range=45,  # randomly rotate images in the range (degrees, 0 to 180)\n",
        "        width_shift_range=0.1,  # randomly shift images horizontally (fraction of total width)\n",
        "        height_shift_range=0.1,  # randomly shift images vertically (fraction of total height)\n",
        "        horizontal_flip=True,  # randomly flip images\n",
        "        vertical_flip=False)  # randomly flip images\n",
        "\n",
        "    for i in range(epoch):\n",
        "      print(\"Loading Latest model weights \")\n",
        "      model = load_model(filepath)\n",
        "      model.fit_generator(train_generator,\n",
        "                          steps_per_epoch=200,\n",
        "                          validation_data=validation_generator, validation_steps=100,\n",
        "                          epochs=50, verbose=1, max_q_size=100,callbacks=callbacks_list)\n",
        "    print(\"Saving model to disk\")\n",
        "    model.save(filepath)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using real-time data augmentation.\n",
            "Loading Latest model weights \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:47: UserWarning: Update your `fit_generator` call to the Keras 2 API: `fit_generator(<keras_pre..., steps_per_epoch=200, validation_data=<keras_pre..., validation_steps=100, epochs=50, verbose=1, callbacks=[<keras.ca..., max_queue_size=100)`\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/keras_preprocessing/image/image_data_generator.py:699: UserWarning: This ImageDataGenerator specifies `featurewise_center`, but it hasn't been fit on any training data. Fit it first by calling `.fit(numpy_data)`.\n",
            "  warnings.warn('This ImageDataGenerator specifies '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "200/200 [==============================] - 394s 2s/step - loss: 4.0358 - acc: 0.4452 - val_loss: 4.5666 - val_acc: 0.3470\n",
            "\n",
            "Epoch 00001: val_acc improved from -inf to 0.34698, saving model to drive/My Drive/weights_folder/weights_best_new2.h5\n",
            "Epoch 2/50\n",
            "200/200 [==============================] - 389s 2s/step - loss: 4.0719 - acc: 0.4321 - val_loss: 4.4832 - val_acc: 0.3523\n",
            "\n",
            "Epoch 00002: val_acc improved from 0.34698 to 0.35233, saving model to drive/My Drive/weights_folder/weights_best_new2.h5\n",
            "Epoch 3/50\n",
            "200/200 [==============================] - 389s 2s/step - loss: 3.9942 - acc: 0.4475 - val_loss: 4.4549 - val_acc: 0.3708\n",
            "\n",
            "Epoch 00003: val_acc improved from 0.35233 to 0.37078, saving model to drive/My Drive/weights_folder/weights_best_new2.h5\n",
            "Epoch 4/50\n",
            "200/200 [==============================] - 389s 2s/step - loss: 3.9366 - acc: 0.4576 - val_loss: 4.4605 - val_acc: 0.3588\n",
            "\n",
            "Epoch 00004: val_acc did not improve from 0.37078\n",
            "Epoch 5/50\n",
            "200/200 [==============================] - 389s 2s/step - loss: 3.8764 - acc: 0.4688 - val_loss: 4.4044 - val_acc: 0.3711\n",
            "\n",
            "Epoch 00005: val_acc improved from 0.37078 to 0.37106, saving model to drive/My Drive/weights_folder/weights_best_new2.h5\n",
            "Epoch 6/50\n",
            "200/200 [==============================] - 389s 2s/step - loss: 3.8521 - acc: 0.4726 - val_loss: 4.3811 - val_acc: 0.3830\n",
            "\n",
            "Epoch 00006: val_acc improved from 0.37106 to 0.38297, saving model to drive/My Drive/weights_folder/weights_best_new2.h5\n",
            "Epoch 7/50\n",
            "200/200 [==============================] - 389s 2s/step - loss: 3.7925 - acc: 0.4841 - val_loss: 4.3782 - val_acc: 0.3736\n",
            "\n",
            "Epoch 00007: val_acc did not improve from 0.38297\n",
            "Epoch 8/50\n",
            "200/200 [==============================] - 390s 2s/step - loss: 3.7549 - acc: 0.4928 - val_loss: 4.3366 - val_acc: 0.3880\n",
            "\n",
            "Epoch 00008: val_acc improved from 0.38297 to 0.38797, saving model to drive/My Drive/weights_folder/weights_best_new2.h5\n",
            "Epoch 9/50\n",
            "200/200 [==============================] - 389s 2s/step - loss: 3.7160 - acc: 0.5004 - val_loss: 4.3185 - val_acc: 0.3813\n",
            "\n",
            "Epoch 00009: val_acc did not improve from 0.38797\n",
            "Epoch 10/50\n",
            "200/200 [==============================] - 389s 2s/step - loss: 3.6826 - acc: 0.5071 - val_loss: 4.3018 - val_acc: 0.3841\n",
            "\n",
            "Epoch 00010: val_acc did not improve from 0.38797\n",
            "Epoch 11/50\n",
            "200/200 [==============================] - 388s 2s/step - loss: 3.6523 - acc: 0.5099 - val_loss: 4.3234 - val_acc: 0.3833\n",
            "\n",
            "Epoch 00011: val_acc did not improve from 0.38797\n",
            "Epoch 12/50\n",
            "200/200 [==============================] - 389s 2s/step - loss: 3.6324 - acc: 0.5173 - val_loss: 4.3214 - val_acc: 0.3887\n",
            "\n",
            "Epoch 00012: val_acc improved from 0.38797 to 0.38870, saving model to drive/My Drive/weights_folder/weights_best_new2.h5\n",
            "Epoch 13/50\n",
            "200/200 [==============================] - 389s 2s/step - loss: 3.6334 - acc: 0.5180 - val_loss: 4.2944 - val_acc: 0.3931\n",
            "\n",
            "Epoch 00013: val_acc improved from 0.38870 to 0.39310, saving model to drive/My Drive/weights_folder/weights_best_new2.h5\n",
            "Epoch 14/50\n",
            "200/200 [==============================] - 389s 2s/step - loss: 3.6532 - acc: 0.5137 - val_loss: 4.3167 - val_acc: 0.3909\n",
            "\n",
            "Epoch 00014: val_acc did not improve from 0.39310\n",
            "Epoch 15/50\n",
            "200/200 [==============================] - 389s 2s/step - loss: 3.6647 - acc: 0.5090 - val_loss: 4.3305 - val_acc: 0.3843\n",
            "\n",
            "Epoch 00015: val_acc did not improve from 0.39310\n",
            "Epoch 16/50\n",
            "200/200 [==============================] - 389s 2s/step - loss: 3.6774 - acc: 0.5043 - val_loss: 4.3362 - val_acc: 0.3741\n",
            "\n",
            "Epoch 00016: val_acc did not improve from 0.39310\n",
            "Epoch 17/50\n",
            "200/200 [==============================] - 390s 2s/step - loss: 3.6649 - acc: 0.5049 - val_loss: 4.3498 - val_acc: 0.3773\n",
            "\n",
            "Epoch 00017: val_acc did not improve from 0.39310\n",
            "Epoch 18/50\n",
            "200/200 [==============================] - 389s 2s/step - loss: 3.7049 - acc: 0.4950 - val_loss: 4.3386 - val_acc: 0.3764\n",
            "\n",
            "Epoch 00018: val_acc did not improve from 0.39310\n",
            "Epoch 19/50\n",
            "200/200 [==============================] - 389s 2s/step - loss: 3.6930 - acc: 0.4986 - val_loss: 4.3432 - val_acc: 0.3723\n",
            "\n",
            "Epoch 00019: val_acc did not improve from 0.39310\n",
            "Epoch 20/50\n",
            "200/200 [==============================] - 388s 2s/step - loss: 3.7056 - acc: 0.4925 - val_loss: 4.3383 - val_acc: 0.3693\n",
            "\n",
            "Epoch 00020: val_acc did not improve from 0.39310\n",
            "Epoch 21/50\n",
            "200/200 [==============================] - 389s 2s/step - loss: 3.7060 - acc: 0.4915 - val_loss: 4.3244 - val_acc: 0.3692\n",
            "\n",
            "Epoch 00021: val_acc did not improve from 0.39310\n",
            "Epoch 22/50\n",
            "200/200 [==============================] - 389s 2s/step - loss: 3.6925 - acc: 0.4909 - val_loss: 4.3104 - val_acc: 0.3722\n",
            "\n",
            "Epoch 00022: val_acc did not improve from 0.39310\n",
            "Epoch 23/50\n",
            "200/200 [==============================] - 390s 2s/step - loss: 3.6641 - acc: 0.4964 - val_loss: 4.3193 - val_acc: 0.3739\n",
            "\n",
            "Epoch 00023: val_acc did not improve from 0.39310\n",
            "Epoch 24/50\n",
            "200/200 [==============================] - 388s 2s/step - loss: 3.6226 - acc: 0.5032 - val_loss: 4.2635 - val_acc: 0.3748\n",
            "\n",
            "Epoch 00024: val_acc did not improve from 0.39310\n",
            "Epoch 25/50\n",
            "200/200 [==============================] - 390s 2s/step - loss: 3.5696 - acc: 0.5124 - val_loss: 4.2203 - val_acc: 0.3878\n",
            "\n",
            "Epoch 00025: val_acc did not improve from 0.39310\n",
            "Epoch 26/50\n",
            "200/200 [==============================] - 390s 2s/step - loss: 3.5445 - acc: 0.5167 - val_loss: 4.2288 - val_acc: 0.3854\n",
            "\n",
            "Epoch 00026: val_acc did not improve from 0.39310\n",
            "Epoch 27/50\n",
            "200/200 [==============================] - 390s 2s/step - loss: 3.4927 - acc: 0.5301 - val_loss: 4.1724 - val_acc: 0.3994\n",
            "\n",
            "Epoch 00027: val_acc improved from 0.39310 to 0.39940, saving model to drive/My Drive/weights_folder/weights_best_new2.h5\n",
            "Epoch 28/50\n",
            "200/200 [==============================] - 390s 2s/step - loss: 3.4656 - acc: 0.5335 - val_loss: 4.2016 - val_acc: 0.3972\n",
            "\n",
            "Epoch 00028: val_acc did not improve from 0.39940\n",
            "Epoch 29/50\n",
            "200/200 [==============================] - 390s 2s/step - loss: 3.4199 - acc: 0.5439 - val_loss: 4.2286 - val_acc: 0.3887\n",
            "\n",
            "Epoch 00029: val_acc did not improve from 0.39940\n",
            "Epoch 30/50\n",
            "200/200 [==============================] - 390s 2s/step - loss: 3.3878 - acc: 0.5505 - val_loss: 4.1482 - val_acc: 0.4047\n",
            "\n",
            "Epoch 00030: val_acc improved from 0.39940 to 0.40469, saving model to drive/My Drive/weights_folder/weights_best_new2.h5\n",
            "Epoch 31/50\n",
            "200/200 [==============================] - 390s 2s/step - loss: 3.3589 - acc: 0.5574 - val_loss: 4.1518 - val_acc: 0.4057\n",
            "\n",
            "Epoch 00031: val_acc improved from 0.40469 to 0.40570, saving model to drive/My Drive/weights_folder/weights_best_new2.h5\n",
            "Epoch 32/50\n",
            "200/200 [==============================] - 390s 2s/step - loss: 3.3349 - acc: 0.5622 - val_loss: 4.1501 - val_acc: 0.3961\n",
            "\n",
            "Epoch 00032: val_acc did not improve from 0.40570\n",
            "Epoch 33/50\n",
            "200/200 [==============================] - 388s 2s/step - loss: 3.3448 - acc: 0.5599 - val_loss: 4.2086 - val_acc: 0.3933\n",
            "\n",
            "Epoch 00033: val_acc did not improve from 0.40570\n",
            "Epoch 34/50\n",
            "200/200 [==============================] - 388s 2s/step - loss: 3.3738 - acc: 0.5515 - val_loss: 4.1557 - val_acc: 0.4024\n",
            "\n",
            "Epoch 00034: val_acc did not improve from 0.40570\n",
            "Epoch 35/50\n",
            "200/200 [==============================] - 388s 2s/step - loss: 3.3624 - acc: 0.5517 - val_loss: 4.1768 - val_acc: 0.3922\n",
            "\n",
            "Epoch 00035: val_acc did not improve from 0.40570\n",
            "Epoch 36/50\n",
            "200/200 [==============================] - 388s 2s/step - loss: 3.4085 - acc: 0.5392 - val_loss: 4.1832 - val_acc: 0.3922\n",
            "\n",
            "Epoch 00036: val_acc did not improve from 0.40570\n",
            "Epoch 37/50\n",
            "200/200 [==============================] - 388s 2s/step - loss: 3.3994 - acc: 0.5427 - val_loss: 4.2095 - val_acc: 0.3870\n",
            "\n",
            "Epoch 00037: val_acc did not improve from 0.40570\n",
            "Epoch 38/50\n",
            "200/200 [==============================] - 388s 2s/step - loss: 3.4269 - acc: 0.5394 - val_loss: 4.2058 - val_acc: 0.3862\n",
            "\n",
            "Epoch 00038: val_acc did not improve from 0.40570\n",
            "Epoch 39/50\n",
            "200/200 [==============================] - 388s 2s/step - loss: 3.4328 - acc: 0.5349 - val_loss: 4.2004 - val_acc: 0.3928\n",
            "\n",
            "Epoch 00039: val_acc did not improve from 0.40570\n",
            "Epoch 40/50\n",
            "200/200 [==============================] - 388s 2s/step - loss: 3.4415 - acc: 0.5284 - val_loss: 4.1964 - val_acc: 0.3917\n",
            "\n",
            "Epoch 00040: val_acc did not improve from 0.40570\n",
            "Epoch 41/50\n",
            "200/200 [==============================] - 388s 2s/step - loss: 3.4470 - acc: 0.5247 - val_loss: 4.2063 - val_acc: 0.3816\n",
            "\n",
            "Epoch 00041: val_acc did not improve from 0.40570\n",
            "Epoch 42/50\n",
            "200/200 [==============================] - 388s 2s/step - loss: 3.4465 - acc: 0.5242 - val_loss: 4.2026 - val_acc: 0.3893\n",
            "\n",
            "Epoch 00042: val_acc did not improve from 0.40570\n",
            "Epoch 43/50\n",
            "200/200 [==============================] - 388s 2s/step - loss: 3.4182 - acc: 0.5278 - val_loss: 4.1534 - val_acc: 0.3859\n",
            "\n",
            "Epoch 00043: val_acc did not improve from 0.40570\n",
            "Epoch 44/50\n",
            "200/200 [==============================] - 388s 2s/step - loss: 3.3716 - acc: 0.5396 - val_loss: 4.1333 - val_acc: 0.3989\n",
            "\n",
            "Epoch 00044: val_acc did not improve from 0.40570\n",
            "Epoch 45/50\n",
            "200/200 [==============================] - 388s 2s/step - loss: 3.3397 - acc: 0.5437 - val_loss: 4.1192 - val_acc: 0.3903\n",
            "\n",
            "Epoch 00045: val_acc did not improve from 0.40570\n",
            "Epoch 46/50\n",
            "200/200 [==============================] - 388s 2s/step - loss: 3.2966 - acc: 0.5524 - val_loss: 4.1207 - val_acc: 0.3945\n",
            "\n",
            "Epoch 00046: val_acc did not improve from 0.40570\n",
            "Epoch 47/50\n",
            "200/200 [==============================] - 388s 2s/step - loss: 3.2547 - acc: 0.5633 - val_loss: 4.0815 - val_acc: 0.3952\n",
            "\n",
            "Epoch 00047: val_acc did not improve from 0.40570\n",
            "Epoch 48/50\n",
            "200/200 [==============================] - 388s 2s/step - loss: 3.2271 - acc: 0.5674 - val_loss: 4.0779 - val_acc: 0.4016\n",
            "\n",
            "Epoch 00048: val_acc did not improve from 0.40570\n",
            "Epoch 49/50\n",
            "200/200 [==============================] - 389s 2s/step - loss: 3.1750 - acc: 0.5766 - val_loss: 4.0505 - val_acc: 0.4118\n",
            "\n",
            "Epoch 00049: val_acc improved from 0.40570 to 0.41184, saving model to drive/My Drive/weights_folder/weights_best_new2.h5\n",
            "Epoch 50/50\n",
            "200/200 [==============================] - 390s 2s/step - loss: 3.1507 - acc: 0.5846 - val_loss: 4.0542 - val_acc: 0.4100\n",
            "\n",
            "Epoch 00050: val_acc did not improve from 0.41184\n",
            "Saving model to disk\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K2D_wcJtcqqp",
        "colab_type": "code",
        "outputId": "a068c5f9-8235-4736-bea5-fe44e12330cd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "val_data = pd.read_csv('./tiny-imagenet-200/val/val_annotations.txt', sep='\\t', header=None, names=['File', 'Class', 'X', 'Y', 'H', 'W'])\n",
        "val_data.drop(['X', 'Y', 'H', 'W'], axis=1, inplace=True)\n",
        "val_data.head(3)\n",
        "train_datagen = ImageDataGenerator(rescale = 1./255)\n",
        "\n",
        "valid_datagen = ImageDataGenerator(rescale=1./255)\n",
        "train_generator = train_datagen.flow_from_directory( r'./tiny-imagenet-200/train/', target_size=(64,64), color_mode='rgb', \n",
        "                                                    batch_size=250, class_mode='categorical', seed=1)\n",
        "validation_generator = valid_datagen.flow_from_dataframe(val_data, directory='./tiny-imagenet-200/val/images/', x_col='File', y_col='Class', target_size=(64,64),\n",
        "                                                    color_mode='rgb', class_mode='categorical', batch_size=64, shuffle=True, seed=42)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 100000 images belonging to 200 classes.\n",
            "Found 10000 images belonging to 200 classes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o1CDBJjcc3pv",
        "colab_type": "code",
        "outputId": "9f4df49e-16d8-4161-95d5-caf1324789c1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 819
        }
      },
      "source": [
        "from keras.callbacks import ReduceLROnPlateau, CSVLogger, EarlyStopping,ModelCheckpoint\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.models import load_model\n",
        "data_augmentation = True\n",
        "batch_size = 128\n",
        "epoch = 1\n",
        "from keras.utils import np_utils\n",
        "#call_backs\n",
        "lr_reducer = ReduceLROnPlateau(factor=np.sqrt(0.1), cooldown=0, patience=5, min_lr=0.5e-6)\n",
        "early_stopper = EarlyStopping(min_delta=0.001, patience=5)\n",
        "filepath=\"drive/My Drive/weights_folder/weights_best_new2.h5\"\n",
        "checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
        "callbacks_list = [checkpoint,clr_triangular,lr_reducer]\n",
        "adm=SGD(lr=0.01, decay=1e-5, momentum=0.95, nesterov=True)\n",
        "model.compile(loss='categorical_crossentropy', metrics=['accuracy'],optimizer=adm)\n",
        "print(\"Loading Latest model weights \")\n",
        "model = load_model(filepath)\n",
        "model.fit_generator(train_generator,\n",
        "                    steps_per_epoch=200,\n",
        "                    validation_data=validation_generator, validation_steps=100,\n",
        "                    epochs=10, verbose=1, max_q_size=100,callbacks=callbacks_list)\n",
        "print(\"Saving model to disk\")\n",
        "model.save(filepath)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loading Latest model weights \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:21: UserWarning: Update your `fit_generator` call to the Keras 2 API: `fit_generator(<keras_pre..., steps_per_epoch=200, validation_data=<keras_pre..., validation_steps=100, epochs=10, verbose=1, callbacks=[<keras.ca..., max_queue_size=100)`\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "200/200 [==============================] - 393s 2s/step - loss: 2.9699 - acc: 0.6279 - val_loss: 3.9134 - val_acc: 0.4391\n",
            "\n",
            "Epoch 00001: val_acc improved from -inf to 0.43906, saving model to drive/My Drive/weights_folder/weights_best_new2.h5\n",
            "Epoch 2/10\n",
            "200/200 [==============================] - 388s 2s/step - loss: 2.9156 - acc: 0.6415 - val_loss: 3.8544 - val_acc: 0.4462\n",
            "\n",
            "Epoch 00002: val_acc improved from 0.43906 to 0.44616, saving model to drive/My Drive/weights_folder/weights_best_new2.h5\n",
            "Epoch 3/10\n",
            "200/200 [==============================] - 388s 2s/step - loss: 2.8190 - acc: 0.6690 - val_loss: 3.9059 - val_acc: 0.4431\n",
            "\n",
            "Epoch 00003: val_acc did not improve from 0.44616\n",
            "Epoch 4/10\n",
            "200/200 [==============================] - 388s 2s/step - loss: 2.8474 - acc: 0.6599 - val_loss: 3.8640 - val_acc: 0.4443\n",
            "\n",
            "Epoch 00004: val_acc did not improve from 0.44616\n",
            "Epoch 5/10\n",
            "200/200 [==============================] - 389s 2s/step - loss: 2.7270 - acc: 0.6954 - val_loss: 3.9088 - val_acc: 0.4359\n",
            "\n",
            "Epoch 00005: val_acc did not improve from 0.44616\n",
            "Epoch 6/10\n",
            "200/200 [==============================] - 388s 2s/step - loss: 2.7630 - acc: 0.6828 - val_loss: 3.8326 - val_acc: 0.4530\n",
            "\n",
            "Epoch 00006: val_acc improved from 0.44616 to 0.45297, saving model to drive/My Drive/weights_folder/weights_best_new2.h5\n",
            "Epoch 7/10\n",
            "200/200 [==============================] - 388s 2s/step - loss: 2.5990 - acc: 0.7319 - val_loss: 3.8700 - val_acc: 0.4539\n",
            "\n",
            "Epoch 00007: val_acc improved from 0.45297 to 0.45387, saving model to drive/My Drive/weights_folder/weights_best_new2.h5\n",
            "Epoch 8/10\n",
            "200/200 [==============================] - 387s 2s/step - loss: 2.6652 - acc: 0.7082 - val_loss: 3.8587 - val_acc: 0.4468\n",
            "\n",
            "Epoch 00008: val_acc did not improve from 0.45387\n",
            "Epoch 9/10\n",
            "200/200 [==============================] - 388s 2s/step - loss: 2.4673 - acc: 0.7702 - val_loss: 3.8854 - val_acc: 0.4414\n",
            "\n",
            "Epoch 00009: val_acc did not improve from 0.45387\n",
            "Epoch 10/10\n",
            "200/200 [==============================] - 388s 2s/step - loss: 2.5483 - acc: 0.7408 - val_loss: 3.8840 - val_acc: 0.4425\n",
            "\n",
            "Epoch 00010: val_acc did not improve from 0.45387\n",
            "Saving model to disk\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fFgV6elgOG3h",
        "colab_type": "code",
        "outputId": "2e94391b-2f04-4420-c577-971febd784ff",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 853
        }
      },
      "source": [
        "from keras.callbacks import ReduceLROnPlateau, CSVLogger, EarlyStopping,ModelCheckpoint\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.models import load_model\n",
        "data_augmentation = True\n",
        "batch_size = 128\n",
        "epoch = 1\n",
        "from keras.utils import np_utils\n",
        "#Y_train = np_utils.to_categorical(y_train, 200)\n",
        "#Y_test = np_utils.to_categorical(y_test, 200)\n",
        "\n",
        "#call_backs\n",
        "lr_reducer = ReduceLROnPlateau(factor=np.sqrt(0.1), cooldown=0, patience=5, min_lr=0.5e-6)\n",
        "early_stopper = EarlyStopping(min_delta=0.001, patience=5)\n",
        "#checkpoint_path = \"drive/My Drive/weights_folder/cp-{epoch:01d}.h5\"\n",
        "#checkpoint_dir = os.path.dirname(checkpoint_path)\n",
        "filepath=\"drive/My Drive/weights_folder/weights_best_new.h5\"\n",
        "#checkpoint_dir = os.path.dirname(filepath)\n",
        "checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
        "callbacks_list = [checkpoint, lr_reducer,clr_triangular]\n",
        "#cp_callback = tf.keras.callbacks.ModelCheckpoint(checkpoint_path, verbose=1, save_weights_only=True,period=2)\n",
        "#model = create_model((64,64,3))\n",
        "\n",
        "\n",
        "if not data_augmentation:\n",
        "    print('Not using data augmentation.')\n",
        "    model.fit(X_train, Y_train,\n",
        "              batch_size=batch_size,\n",
        "              epochs=epoch,\n",
        "              validation_data=(X_test, Y_test),\n",
        "              shuffle=True,\n",
        "              callbacks=callbacks_list)\n",
        "else:\n",
        "    print('Using real-time data augmentation.')\n",
        "    # This will do preprocessing and realtime data augmentation:\n",
        "    datagen = ImageDataGenerator(\n",
        "        featurewise_center=True,  # set input mean to 0 over the dataset\n",
        "        samplewise_center=True,  # set each sample mean to 0\n",
        "        featurewise_std_normalization=False,  # divide inputs by std of the dataset\n",
        "        samplewise_std_normalization=False,  # divide each input by its std\n",
        "        zca_whitening=False,  # apply ZCA whitening\n",
        "        rotation_range=45,  # randomly rotate images in the range (degrees, 0 to 180)\n",
        "        width_shift_range=0.1,  # randomly shift images horizontally (fraction of total width)\n",
        "        height_shift_range=0.1,  # randomly shift images vertically (fraction of total height)\n",
        "        horizontal_flip=True,  # randomly flip images\n",
        "        vertical_flip=False)  # randomly flip images\n",
        "\n",
        "    # Compute quantities required for featurewise normalization\n",
        "    # (std, mean, and principal components if ZCA whitening is applied).\n",
        "    #datagen.fit(X_train)\n",
        "    \n",
        "    # Fit the model on the batches generated by datagen.flow().\n",
        "    #model.load_weights(latest)\n",
        "    #model.fit_generator(datagen.flow(X_train, Y_train, batch_size=batch_size),\n",
        "    #                      steps_per_epoch=X_train.shape[0] // batch_size,\n",
        "    #                     validation_data=(X_test, Y_test),\n",
        "    #                     epochs=1, verbose=1,callbacks=callbacks_list)\n",
        "    \n",
        "    #model = load_model(filepath)\n",
        "    for i in range(epoch):\n",
        "      print(\"Loading Latest model weights \")\n",
        "      model = load_model(filepath)\n",
        "      #model.fit_generator(train_generator, epochs=20, steps_per_epoch=500, validation_steps=100, validation_data=validation_generator)\n",
        "      model.fit_generator(train_generator,\n",
        "                            steps_per_epoch=200,\n",
        "                            validation_data=validation_generator, validation_steps=100,\n",
        "                            epochs=10, verbose=1, max_q_size=100,callbacks=callbacks_list)\n",
        "      #model.compile(loss='categorical_crossentropy', metrics=['accuracy'],optimizer=adm)\n",
        "      #score = model.evaluate_generator(validation_generator,steps=10, verbose=1)\n",
        "      #print(\"%s: %.2f%%\" % (model.metrics_names[1], score[1]*100))\n",
        "      print(\"Saving model to disk\")\n",
        "      model.save(filepath)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using real-time data augmentation.\n",
            "Loading Latest model weights \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:66: UserWarning: Update your `fit_generator` call to the Keras 2 API: `fit_generator(<keras_pre..., steps_per_epoch=200, validation_data=<keras_pre..., validation_steps=100, epochs=10, verbose=1, callbacks=[<keras.ca..., max_queue_size=100)`\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/keras_preprocessing/image/image_data_generator.py:699: UserWarning: This ImageDataGenerator specifies `featurewise_center`, but it hasn't been fit on any training data. Fit it first by calling `.fit(numpy_data)`.\n",
            "  warnings.warn('This ImageDataGenerator specifies '\n",
            "/usr/local/lib/python3.6/dist-packages/keras_preprocessing/image/image_data_generator.py:707: UserWarning: This ImageDataGenerator specifies `featurewise_std_normalization`, but it hasn't been fit on any training data. Fit it first by calling `.fit(numpy_data)`.\n",
            "  warnings.warn('This ImageDataGenerator specifies '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "200/200 [==============================] - 426s 2s/step - loss: 1.4088 - acc: 0.8018 - val_loss: 2.8421 - val_acc: 0.4911\n",
            "\n",
            "Epoch 00001: val_acc improved from -inf to 0.49109, saving model to drive/My Drive/weights_folder/weights_best_new.h5\n",
            "Epoch 2/10\n",
            "200/200 [==============================] - 421s 2s/step - loss: 1.3282 - acc: 0.8235 - val_loss: 2.8523 - val_acc: 0.4948\n",
            "\n",
            "Epoch 00002: val_acc improved from 0.49109 to 0.49480, saving model to drive/My Drive/weights_folder/weights_best_new.h5\n",
            "Epoch 3/10\n",
            "200/200 [==============================] - 421s 2s/step - loss: 1.2984 - acc: 0.8306 - val_loss: 2.8566 - val_acc: 0.4980\n",
            "\n",
            "Epoch 00003: val_acc improved from 0.49480 to 0.49795, saving model to drive/My Drive/weights_folder/weights_best_new.h5\n",
            "Epoch 4/10\n",
            "200/200 [==============================] - 421s 2s/step - loss: 1.2459 - acc: 0.8452 - val_loss: 2.8379 - val_acc: 0.4972\n",
            "\n",
            "Epoch 00004: val_acc did not improve from 0.49795\n",
            "Epoch 5/10\n",
            "200/200 [==============================] - 421s 2s/step - loss: 1.2572 - acc: 0.8393 - val_loss: 2.8884 - val_acc: 0.4957\n",
            "\n",
            "Epoch 00005: val_acc did not improve from 0.49795\n",
            "Epoch 6/10\n",
            "200/200 [==============================] - 421s 2s/step - loss: 1.2090 - acc: 0.8544 - val_loss: 2.8298 - val_acc: 0.5031\n",
            "\n",
            "Epoch 00006: val_acc improved from 0.49795 to 0.50315, saving model to drive/My Drive/weights_folder/weights_best_new.h5\n",
            "Epoch 7/10\n",
            "200/200 [==============================] - 421s 2s/step - loss: 1.2153 - acc: 0.8522 - val_loss: 2.8645 - val_acc: 0.5038\n",
            "\n",
            "Epoch 00007: val_acc improved from 0.50315 to 0.50375, saving model to drive/My Drive/weights_folder/weights_best_new.h5\n",
            "Epoch 8/10\n",
            "200/200 [==============================] - 421s 2s/step - loss: 1.1625 - acc: 0.8663 - val_loss: 2.8618 - val_acc: 0.5091\n",
            "\n",
            "Epoch 00008: val_acc improved from 0.50375 to 0.50913, saving model to drive/My Drive/weights_folder/weights_best_new.h5\n",
            "Epoch 9/10\n",
            "200/200 [==============================] - 421s 2s/step - loss: 1.1545 - acc: 0.8682 - val_loss: 2.8471 - val_acc: 0.5022\n",
            "\n",
            "Epoch 00009: val_acc did not improve from 0.50913\n",
            "Epoch 10/10\n",
            "200/200 [==============================] - 421s 2s/step - loss: 1.1091 - acc: 0.8810 - val_loss: 2.8532 - val_acc: 0.5092\n",
            "\n",
            "Epoch 00010: val_acc improved from 0.50913 to 0.50922, saving model to drive/My Drive/weights_folder/weights_best_new.h5\n",
            "Saving model to disk\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BnjkaakIXdKi",
        "colab_type": "code",
        "outputId": "14cdb8bf-29fc-407f-c832-342df2c1ef5a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 541
        }
      },
      "source": [
        "#from keras.callbacks import History \n",
        "#Get training and test loss histories\n",
        "\n",
        "training_loss = model.history.history['loss']\n",
        "test_loss = model.history.history['val_loss']\n",
        "\n",
        "# Create count of the number of epochs\n",
        "epoch_count = range(1, len(training_loss) + 1)\n",
        "\n",
        "# Visualize loss history\n",
        "plt.plot(epoch_count, training_loss, 'r--')\n",
        "plt.plot(epoch_count, test_loss, 'b-')\n",
        "plt.legend(['Training Loss', 'Test Loss'])\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.show();\n",
        "\n",
        "train_accuracy = model.history.history['acc']\n",
        "val_accuracy = model.history.history['val_acc']\n",
        "epoch_count = range(1, len(val_accuracy) + 1)\n",
        "plt.plot(epoch_count, train_accuracy, 'b-')\n",
        "plt.plot(epoch_count, val_accuracy, 'r-o')\n",
        "plt.legend(['train_accuracy', 'val_accuracy'])\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.show();"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEGCAYAAACKB4k+AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ngg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEGCAYAAACKB4k+AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n3kCNRwAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W_A9xTkifdqE",
        "colab_type": "code",
        "outputId": "9038a40c-c588-485d-a1be-2d10717fafa9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 311
        }
      },
      "source": [
        "plt.xlabel('Training Iterations')\n",
        "plt.ylabel('Learning Rate')\n",
        "plt.title(\"CLR - 'triangular' Policy\")\n",
        "plt.plot(clr_triangular.history['iterations'], clr_triangular.history['lr'])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7f88e89a7cc0>]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZEAAAEVCAYAAAAhANiZAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n7ATlXEjeuEgwFQAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kU4ZRPqT8u6z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class_dictionary = validation_generator.class_indices"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XiNul6sV9Fv-",
        "colab_type": "code",
        "outputId": "e91036ec-17c0-4fa9-893d-d8503ca471b4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 163
        }
      },
      "source": [
        "model.evaluate_generator(generator=validation_generator,steps=1000)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-7df7630d0ff0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate_generator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenerator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_generator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msteps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gve1cTmv9IS4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "validation_generator.reset()\n",
        "pred=model.predict_generator(validation_generator,verbose=1)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}